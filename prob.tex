%!TEX root =  flsa.tex
\section{Problem Setup}\label{sec:prob}
We consider linear stochastic approximation algorithm (LSAs) with constant step-size (CS) and Polyak-Ruppert (PR) averaging of the iterates given as below:
\begin{subequations}\label{eq:lsa}
\begin{align}
\label{conststep}&\text{LSA:} &\theta_t&=\theta_{t-1}+\alpha(b_t-A_t\theta_{t-1})\,,\\
\label{iteravg}&\text{PR-Average:} &\thh_t&=\frac{1}{t+1}{\sum}_{i=0}^{t}\,\theta_i\,.
\end{align}
\end{subequations}
The algorithm updates a pair of parameters $\theta_t,\tb_t\in \R^{d}$ incrementally, in discrete time steps $t=1,2,\dots$
based on data $b_t\in \R^d$, $A_t\in \R^{\dcd}$. Here $\alpha>0$ is a positive step-size parameter; the only tuning parameter of the algorithm besides the
initial value $\theta_0$. The iterate $\theta_t$ is treated as an internal state of the algorithm, while $\thh_t$ is the output at time step $t$. The update of $\theta_t$ alone is considered a form of constant step-size LSA.

Note that the computation of $\theta_t$ can also be performed in an incremental fashion using $O(d)$ storage. 
Moreover, sometimes $A_t$ will have a special form and then the matrix-vector product $A_t \theta_{t-1}$ can also be computed in $O(d)$ time. This happens for example when $A_t$ is rank one.
When the data $(b_t,A_t)$ is sparse, further speedups are possible.
Examples of this kind arise in reinforcement learning \cite{sutton,konda-tsitsiklis,gtd,gtd2,gtdmp}, 
for details see \cref{sec:related}.
The significance of efficient computation of the matrix-vector product is that an update of the algorithm
can then be implemented in $O(d)$ time (or less, in case of sparse data) and $O(d)$ storage, which makes the algorithm particularly attractive in large-scale computations when $d$ is in the range of thousands, or millions, or more, as may be required by modern applications (e.g., \citep{LiMaTaBo16})

In what follows, for $t\ge1$ we make use of the $\sigma$-fields $\F_{t-1}\eqdef\{\theta_0,A_1,\ldots, A_{t-1}, b_1,\ldots, b_{t-1}\}$; $\F_{-1}$ is the trivial $\sigma$ algebra. \todoc{You wrote $\F_0$ holds all random variables, an unusual choice. And it does not work, I think and the index should be $-1$. The martingale property breaks otherwise with the first time step.}
We are interested in the behaviour of \eqref{eq:lsa} under the following assumption:
\begin{assumption}\label{assmp:lsa}
\begin{enumerate}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item \label{dist} $(b_t, A_t)\sim P$, $t\geq 0$ is an \iid sequence.
We let $A_P$ be the expectation of $A_t$, $b_P$ be the expectation of $b_t$, as in \cref{def:dist}.
%where $P^b$ is a distribution over $\R^d$ and $P^A$ is a distribution over $\R^{\dcd}$. 
\todoc{I got rid off $P^b$ and $P^A$. There were $P^V$ and $P^M$ previously.. Confusing.}
We assume that $P$ is Hurwitz.
\item \label{matvar} The martingale difference sequences\footnote{That is, $\EE{M_t|\F_{t-1}}=0$ and $\EE{N_t|\F_{t-1}}=0$ and $M_t,N_t$ are $\F_t$ measurable, $t\ge 0$.} $M_t\eqdef A_t-A_{P}$ and $N_t\eqdef b_t-b_{P}$ associated with $A_t$ and $b_t$ satisfy the following 
\todoc{So there is conditioning for $M_t$, and no conditions for $N_t$? Interesting. Maybe comment on this
that this is not a mistake.
Also, why write $N_t^* N_t$ instead of $\norm{N_t}^2$???}
\todoc{This would be the place to state if we make assumptions above the correlations between $M_t$ and $N_t$.}
\begin{align*}
	\E\left[ \norm{M_t}^2\mid\F_{t-1}\right]\leq \sigma^2_{A_P}\,, \qquad 
	\E[N_t^* N_t]=\sigma^2_{b_P}\,.
\end{align*}
with some $\sigma^2_{A_P}$ and $\sigma^2_{b_P}$. \todoc{Could not these just be $\sigma^2_A$, $\sigma^2_b$, or with $M$ and $V$?}
\item $A_P$ is invertible and thus the vector $\ts=A^{-1}_Pb_P$ is well-defined. \todoc{Bertsekas looked at the case when $\ts$ is well-defined but $A_P$ is not invertible. Future work..?}
\end{enumerate}
\end{assumption}
%For the rest of the paper we denote the errors of the internal and the output variables of \eqref{eq:lsa} at time $t$ by $e_t\eqdef \theta_t-\ts$ and $\eh_t\eqdef \thh_t-\ts$ respectively.
\paragraph{Performance Metric:}  
We are interested in the behavior of the mean squared error (MSE) at time $t$ given%
\footnote{More generally, one can be interested in $\EEP{\normsm{\thh_t-\ts}_C^2}$, the MSE with respect to a PD Hermitian matrix $C$. Since in general it is not possible to exploit the presence of $C$ unless it is connected to $P$ in a special way,
here we restrict ourselves to $C = \I$. For more discussion, including the discussion of the case of SGD for linear least-squares when $C$ and $P$ are favourably connected see \cref{sec:related}.}
 by $\EE{\normsm{\thh_t-\ts}^2}$. \todoc{Move up footnote. Polish..}
\begin{comment}
\footnote{We note here that $\E[\normsm{\thh_t-\ts}_C^2]$, which measures the MSE in the quadratic norm with respect to a positive definite matrix $C\succ 0$. However, unless $C$ has some special structure, it is not possible to exploit this generality in our bounds. Further, when data is bounded $\norm{C}^2$ is also bounded and it is straightforward to see that $\EEP{\norm{\thh_t-\ts}}^2_C\leq \norm{C}^2\EEP{\norm{\thh_t-\ts}^2}$. So, for the purpose of stating our results we will not use the general quadratic norm, however, will use it when we discuss the work by \cite{bach} in LSE, where its usage is relevant.}
\end{comment}
%We wish to understand how the MSE is affected by the choice of $\alpha$ and the spectral properties of $A_P$. Another aspect of interest is to understand the contribution of the \emph{bias} (caused by the initial condition $\theta_0$) and the \emph{variance} (caused by $\sigma^2_P,\sigma^2_b$) to the MSE.\par
\todoch{Mention importance of forgetting the bias etc. Mention that we really don't have control over $\norm{U}$}

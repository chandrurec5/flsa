\section{Introduction}
A linear stochastic approximation (LSA) algorithm is given by
\begin{align}\label{eq:lsaintro}
\theta_t=\theta_{t-1}+\alpha_t (b_t-A_t \theta_{t-1}),
\end{align}
where the iterates, $\theta_t\in \R^d$, and step-sizes, $\alpha_t>0$, are algorithmic quantities, and $(b_t,A_t)\in \R^d\times \R^{\dcd}$ is the noisy data sequence presented to the algorithm.
LSA algorithms (or simply LSAs) are useful in machine learning and reinforcement learning (RL) applications, where the aim is to compute a $\ts\in \R^d$ using only the noisy data sequence and $O(d)$ updates per iteration. Some examples of LSAs include the stochastic gradient descent algorithm (SGD) for the problem of linear regression with quadratic loss \cite{bach,bachaistats}, and the temporal difference class (TD) of learning algorithms in RL \cite{sutton,konda-tsitsiklis,gtd,gtd2,gtdmp}.\par
%An additional feature in these class of applications is that $A_t$ turns out to be a rank-$1$ matrix and $A_t\theta_{t-1}$ can be obtained in $O(d)$, which is attractive due to the cheap per time step computational requirement.\par

The choice of the step-sizes $\alpha_t$ is critical for the performance of the LSA in \eqref{eq:lsaintro}.
Informally speaking, smaller step-sizes are better for noise rejection and larger step-sizes lead to faster learning, while at the same time too large step-sizes might result in instability of \eqref{eq:lsaintro}. A useful choice in RL has been the diminishing and square-summable step-sizes \cite{gtd2,borkarbook,konda-tsitsiklis}, where $\alpha_t\ra 0$ such that $\sum_{t\geq 0} \alpha_t=\infty$, $\sum_{t\geq 0}\alpha^2_t<\infty$. An alternate technique, which we call LSA with constant step-size Polyak-Ruppert (CS-PR) averaging, is to run \eqref{eq:lsaintro} as a constant step-size LSA by choosing $\alpha_t=\alpha>0,\forall t\geq 0$, and output the average $\thh_t\eqdef\frac{1}{t+1}\sum_{s=0}^t \theta_t$. In LSA with CS-PR, $\theta_t$ is an internal variable and $\thh_t$ is the output of the algorithm. The idea of averaging in stochastic approximation to improve noise rejection is a technique that goes back to  \citet{ruppert,polyak-judisky}. Intuitively, the CS-PR technique can be seen to address the learning vs noise rejection trade-off, where the constant step-size takes care of the learning  and the averaging of the iterates takes care of the noise rejection.
\par
Recently, \citet{bach} considered SGD\footnote{SGD is a special of the LSA algorithm in \eqref{eq:lsaintro}.} with CS-PR for the linear regression problem with quadratic loss and $i.i.d$ sampling. A ``remarkable'' result by \citet{bach} is that there exists a constant step-size such that for any problem where the noisy data is bounded by a known constant, the expected squared regression error after $t$ updates is at most $\frac{C}{t}$ with a constant $C>0$ that depends \emph{only} on the bound on the data.\par\todoc{Dimension?}

\paragraph{Motivation and Contribution:} In this paper, we wish to study LSA with CS-PR where data $(b_t,A_t)$ is $i.i.d$ with some underlying distribution $P$ such that variance is bounded. An additional restriction we place on $P$ is that the matrix $\EEP{A_t}$ is Hurwitz, i.e., all its eigenvalues have positive real parts. For a given $P$, we wish to measure the performance of LSA with CS-PR in terms of the mean square error (MSE) $\EEP{\norm{\thh_t-\ts}^2}$. Further, for a given class of distributions $\P$, we would like to know whether it is possible to ensure uniform performance $\forall P\in \P$.
Our study is motivated by the problem \emph{policy evaluation} \cite{dann} using linear value function approximation from experience replay \cite{lin} in a batch setting \cite{lange} in RL using TD class  of algorithms \cite{sutton,konda-tsitsiklis,gtd,gtd2,gtdmp}. Here, we wish to understand the effectiveness of the CS-PR technique for the TD class of LSA algorithms used in RL.

We now list our contributions in this paper
%While asymptotically diminishing step-sizes are common in RL literature \cite{ilstd,gtdmp,korda-prashanth}, the constant step-size choice has also been used in the past \cite{gtd2}.
%\begin{itemize}[leftmargin=*]%, before = \leavevmode\vspace{-\baselineskip}]
\paragraph{Instance Dependent Bounds} We show that for a fixed $P$, there exists a $\alpha_P>0$ such that the mean squared error (MSE) of the LSA with CS-PR is at most $O(\frac{1}{t})$, whenever $\alpha\in (0,\alpha_P)$. The MSE can further be broken down to a \emph{bias} term (arising due to the initial condition $\theta_0$) that decays at a rate $O(\frac{1}{t^2})$ and a \emph{variance} term (due to the noise) that decays at at rate $O(\frac{1}{t})$.
\paragraph{Uniform Step-Sizes:} We also show (via counter example) that there is a class $\P$ that does not `admit' a single constant step-size $\alpha_{\P}$ that guarantees uniform performance $\forall P\in \P$. This is a negative result in the context of RL. However,
we also argue that the subclass $\P_{LR}$ related to SGD for linear regression does `admit' a problem independent constant step-size, thereby recovering a part of the result by \citet{bach}.
\paragraph{Automatic Step-Size:} We argue informally that for any given $P$, a constant step-size can be chosen in an `automated' fashion, and the performance of CS-PR is comparable to other popular step-size choices used for LSA in RL literature.
\paragraph{Organization:}

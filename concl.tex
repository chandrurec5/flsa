\section{Conclusion}
We presented a finite time performance analysis of LSAs with CS-PR and showed that the MSE decays at a rate $O(\frac{1}{t})$. Our results extended the analysis of \citet{bach} for SGD with CS-PR for the problem of linear regression with quadratic loss and \iid sampling to general LSAs with CS-PR. Our results were directly applicable to TD class of algorithms that are LSAs. In particular, new performance bounds for TDC follows from our results. Our results also improved the rates in the case of the GTD class of algorithms. We presented conditions under which a constant step-size can be chosen uniformly for a give class of data distributions. We showed a negative result in that not all data distributions `admit' such a constant step-size. This is a negative result from the perspective of TD algorithms in RL. We also argued heuristically that a constant step-size can be obtained in an automatic manner for a given LSA and a given data distribution. Thus, LSAs with CS-PR technique $i)$ achieves comparable or better error rates $ii)$ alongside automatic tuning of the constant step-size can perform comparably well in practice.

\section{Related Work}
We first discuss the related work outside of RL setting, followed by related work in the RL setting. In both cases, we highlight the new insights that follows from the results in this paper.\par
\paragraph{General LSA:} Analysis of LSA with CS-PR goes back to the work by \citet{polyak-judisky}, wherein they considered the additive noise setting i.e., $A_t=A$ for some deterministic Hurwitz matrix $A\in \R^{\dcd}$. A key improvement in this paper is that we consider the `multiplicative' noise case, i.e., $A_t$ is a random matrix.
\paragraph{SGD for Linear Regression} As mentioned in \Cref{sec:intro}, we were motivated by the ``remarkable'' results obtained by \citet{bach}, where the authors analyse the performance of SGD for the problem of linear regression with quadratic loss.
\begin{comment}
Here, $(x_t,y_t)\in \R^d\times \R,\,t\geq 1$ is an $i.i.d$ a bounded (by $B$) data sequence such that $\EE{\norm{x_t}^2}$ and $\EE{y_t^2}$ are finite. $x_t,\,t\geq 1$ are the features and $y_t$ are real labels and the linear regression problem constitutes minimizing the quadratic loss function $f(\theta)=\EE{\left(\ip{x_t,\theta}-y_t\right)^2}$. SGD with CS-PR to minimize $f(\theta)$ can be given as below:
\begin{subequations}\label{eq:linreg}
\begin{align}
\theta_t&=\theta_{t-1}-\alpha x_t\left(\ip{x_t,\theta_{t-1}} -y_t\right)\\
\thh_t&=\frac{1}{t+1}\sum_{s=0}^t \theta_t
\end{align}
\end{subequations}
Here $\nabla f(\theta_{t-1})=x_t\left(\ip{x_t,\theta_{t-1} -y_t}\right)$ is the \emph{stochastic gradient} of the objective $f(\theta)$. It follows from the $i.i.d$ assumption on $(x_t,y_t)$ that $A_t=x_t x_t^\top$ and $b_t=y_t x_t$ are also $i.i.d$, and with this identification one can see that \eqref{linreg} is of the form in \eqref{conststep}.
%In this case $A_P$ is SPD.
In the linear regression setting, the prediction error is given by $\EE{\norm{\theta-\ts}^2_{A_P}}$.
%$(ii)$ the MSE in \eqref{eq:mselinreg} is measured with respect to $A_P$, $(iii)$ the noise $%\epsilon_t=y_t-\ip{\ts,x_{t-1}}$ is structured i.e., $\EE{\epsilon_t^2 x_t x_t^\top}\leq R A_P$ for some constant $R>0$.
\end{comment}
The problem class considered by them belongs to $\P_{\text{PSD},B}$ (see \Cref{def:pclass} ) and hence is weakly admissible with a uniform step-size choice. In order to produce a rate of $\frac{C}{t}$ (where $C>0$ is a problem independent constant) for the bias term \citet{bach} do not rely on admissibility, but instead use the fact that the MSE $\EE{\norm{\thh_t-\ts}^2_{A_P}}$ for the linear prediction problem is measured in the general quadratic norm induced by $A_P$ and the fact that the noise is `structured' i.e., the noise is bounded by $R\,A_P$ for some constant $R>0$. At the same time, the bias term decays at a rate $\frac{C'}{t^2}$, where the constant $C'$ contains the factor $(\theta_0-\ts)^\top A^{-1}_P (\theta_0-\ts)$ and is dependent on the condition number.
\subsection{Reinforcement Learning}
\paragraph{Notation:} We are presented with data in the form of an $i.i.d$ sequence $(\phi_t,\phi'_t,r_t)\in \R^d\times\R^d\times \R$. Let $\gamma \in (0,1)$ be a fixed constant, we define $\Delta_t\eqdef \phi_t\phi_t^\top-\gamma \phi_t\phi_t^{'\top}$, $C_t=\phi_t\phi_t^\top$ and $b_t=\phi_r r_t$. In what follows, $\mu_t>0$ is an \emph{importance} sampling factor whose aim is to correct for mismatch in the (behavior) distribution with which the data was collected and the (target) distribution with respect to which one wants to learn. A factor $\mu_t=1,\,\forall t\geq 0$ will mean that no correction is required\footnote{This is known as the \emph{on-policy} case where the behavior is identical to the target. The general setting where $\mu_t>0$ is known as \emph{off-policy}.}.
\FloatBarrier
\begin{table}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|l|p{60mm}|}\hline
Algorithm& Update & Remark\\ \hline
TD(0)& $\begin{aligned}\theta_t&=\theta_{t-1}+\alpha_t (b_t -\Delta_t\theta_{t-1})\end{aligned}$ & \cite{korda-prashanth}:$\alpha_t=O(\frac{1}{t})^\beta,\beta\in(0,1)$; PR-avg; $\EE{\norm{\eh_t}}= O(\frac{1}{\sqrt{t}}).$ \\ \hline
GTD/GTD2& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}- Q_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t A^\top_t y_{t-1})\end{aligned}$
& \cite{gtd2}: $\beta_t=\eta \alpha_t,\sum_{t\geq 0}\alpha_t=\infty, \sum_{t\geq 0}\alpha^2_t<\infty$; $e_t\ra 0$ as $t\ra\infty$ w.p.1. \cite{gtdmp}:
$\alpha_t=\beta_t=O(\frac{1}{\sqrt{t}})$; Projection+PR; $\,e_t =O(\frac{1}{{t}^{\frac{1}{4}}})$ w.h.p.\\\hline
TDC& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}-C_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t b_t-\mu_t\Delta_t\theta_{t-1}+ (\Delta^\top_t-C_t)y_{t-1})\end{aligned}$ & \cite{gtd2}: $\beta_t=\eta \alpha_t,\sum_{t\geq 0}\alpha_t=\infty, \sum_{t\geq 0}\alpha^2_t<\infty$; $e_t\ra 0$ as $t\ra\infty$ w.p.1. \\\hline
\end{tabular}
}
\label{tb:rates}
\caption{Some popular TD algorithms with rates known in literature \cite{korda-prashanth,gtd2,gtdmp}. }
\end{table}
\begin{comment}
\begin{table}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}\hline
Algorithm& Update &Stepsize &Rate \\ \hline
TD(0)& $\begin{aligned}\theta_t&=\theta_{t-1}+\alpha_t (b_t -\Delta_t\theta_{t-1})\\ \thh_t&=\frac{1}{t+1}\sum_{s=0}^t \theta_t\end{aligned}$ &$\begin{aligned}\alpha_t&=O(\frac1{t})^\delta,\\ &\delta\in(1/2,1)\end{aligned}$ &$\EE{\norm{\eh_t}}\leq O(\frac{1}{\sqrt{t}})$ for $\delta\ra 1$ \\ \hline
GTD& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}- Q_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t A^\top_t y_{t-1})\end{aligned}$ & $\begin{aligned}&\beta_t=\eta \alpha_t,\sum_{t\geq 0}\alpha_t=\infty, \\ &\sum_{t\geq 0}\alpha^2_t<\infty \end{aligned}$ & $e_t\ra 0$ w.p.1 \\\hline
TDC& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}-C_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t b_t-\mu_t\Delta_t\theta_{t-1}+ (\Delta^\top_t-C_t)y_{t-1})\end{aligned}$ & $\begin{aligned}&\beta_t,\alpha_t,\frac{\alpha_t}{\beta_t} \ra 0,~\text{as}~t\ra \infty,\\& \sum_{t\geq 0}\alpha_t=\infty, \sum_{t\geq 0}\beta_t=\infty, \\ &\sum_{t\geq 0}\alpha^2_t<\infty, \sum_{t\geq 0}\beta^2_t<\infty \end{aligned}$ & $e_t\ra 0$ w.p.1 \\\hline
GTD& $\begin{aligned} y_{t}&=\Pi_Y \left(y_{t-1}+\alpha_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}-Q_t y_{t-1})\right)\\ \theta_t &=\Pi_{\Theta}\left(\theta_{t-1}+\alpha_t (\mu_t A_t^\top y_{t-1})\right)\\ \thh_t&=\frac{\sum_{s=0}^t \alpha_s\theta_s}{\sum_{s=0}^t \alpha_s}\end{aligned}$ & $\alpha_t =O(\frac{1}{\sqrt{t}})$ & $\norm{A_P\eh_t-b}^2 \leq O(\frac{1}{\sqrt{t}})$ w.h.p\\\hline

\end{tabular}
}
\label{tb:rates}
\end{table}
\end{comment}
Implications
\begin{itemize}
\item It follows that for each instance of blah blah
\item Bias vairance trade off
\end{itemize}
\begin{comment}
\paragraph{Related Work in RL} The various temporal difference learning algorithms can be written as in \Cref{table:td}. The TD(0) algorithm is an on-policy algorithm ($\mu_t=1$) and is unstable in the general {off-policy} setting. The gradient temporal difference (GTD) algorithm \cite{gtd,gtd2,gtdmp} is stable in off-policy setting as well. Recent work by \cite{korda-prashanth} derive finite time bounds for the TD(0) for two different cases of step-sizes one without PR averaging and other with PR averaging. In the case without PR averaging, \cite{korda-prashanth} consider step-sizes of the form $\frac{c_0c}{c+t}$, where $c_0$ and $c$ are problem dependent positive constants, to dervie a rate of $O(\frac{1}{t})$ for the MSE.  In the case with PR averaging (row $1$ of \Cref{tb:rates}), \cite{korda-prashanth} consider step-size of the from $c_0\left(\frac{c}{c+t}\right)^\delta,\,\delta \in (1/2,1)$ where $c_0, c$ are problem independent positive constants, and show that this choice yeilds a rate of $O(\frac{1}{t})$ for $\delta \ra 1$ and $t\geq t_0$ (where $t_0$ is a problem dependent time instant). A possible limiting feature of the results by \cite{korda-prashanth} is that either the problem knowledge is required or the bounds hold only after a problem dependent time. A centered TD(0) variant is also considered by \cite{korda-prashanth}, however, a projection operator is required to keep the iterates bounded. Also, the bounds by \cite{korda-prashanth} don't have an explicit bias variance decomposition. The works on GTD and TDC by \cite{gtd,gtd2} show only asymptotic convergence w.p.1. Recently, the work by \cite{gtdmp} shows a rate of $\frac{1}{\sqrt{t}}$ for GTD with use of projection operation, and also proposed a variant namely GTD-Mirror-prox based on stochastic mirror-prox ideas which achieves $\frac{1}{\sqrt{t}}$ rate without use of projection. In comparison, we don't use of projections, our bounds hold for all time $t$, and the bounds are dependent on the problem instance only to the extent of choosing a step-size that leads to stable behavior, in which case we show that $O\frac{1}{t}$ rate holds (the scaling factor hidden in the $O$ expression is problem dependent). Further, our results provide explicit bias variance decomposition.
\end{comment}
An interesting fact to note is while theoretical analysis for GTD/GTD2/TDC are presented for the case of diminishing step-size, the experimental results are reported for constant step-size choices \cite{gtd2,gtdmp}.

\appendix
\section{Linear Algebra Preliminaries}
We now present some useful results from linear algebra.
\subsection{Notation}
Let $B$ be a $\dcd$ block diagonal matrix given by $B=\begin{bmatrix} B_1 &0 &0 &\ldots &0 \\ 0 &B_2 &0 &\ldots &0  \\ \vdots &\vdots &\vdots &\vdots &\vdots \\ 0 &\ldots &0 &0 &B_k \end{bmatrix}$, where $B_i$ is a $n_i \times n_i$ matrix such that $n_i<n$ (w.l.o.g) and $\sum_{i=1}^k n_i=n$. We also denote $B$ as
\begin{align*}
B=B_1 \op B_2 \op \ldots B_k=\op_{i=1}^k B_i
\end{align*}

\subsection{Results in Matrix Decomposition and Transformation}
We will now recall Jordon decomposition.

\begin{comment}
\begin{lemma}
Given a matrix $A\in \C^{\dcd} there exists a \emph{characteristic} polynomial denoted by $\chi_A=(\lambda_1 \I-A)^{n_1}\ldots(\lambda_{k} \I-A)^{n_k}$, where $\lambda_i,\,i=1,\ldots,k (k\leq n)$ are the $k$ distinct eigenvalues of the matrix $A$ such that $\sum_{i=1}^k n_i=n$.
\end{lemma}
\begin{lemma}
There exists a polynomial known as the \emph{minimal} polynomial denoted by $m_A=(\lambda_1 \I-A)^{n'_1}\ldots(\lambda_k \I-A)^{n'_k}$, where $n'_i\leq n_i,\,i=1\ldots,k$.
\end{lemma}
\end{comment}

\begin{lemma}\label{jordon}
Let $A\in \C^{\dcd}$ and $\{\lambda_i\in \C,i=1,\ldots,k\leq n \}$ denote its $k$ distinct eigenvalues.
There exists a complex matrix $V\in \C^{\dcd}$ such that $A=V\tL V^{-1}$, where
$\tL=\tL_1\op\ldots\op\tL_k$, where each $\tL_i,\,i=1,\ldots,k$ can further be written as $\tL_i= {\tL}^i_{1}\op \ldots \op {\tL}^i_{{l(i)}}$. Each of ${\tL}^i_{j},j=1,\ldots,l(i)$ is a $d^i_j\times d^i_j$ square matrix such that $\sum_{j=1}^{l(i)} d^i_j =n_i$ and has the special form given by
${\tL}^i_{j}=\begin{bmatrix} \lambda_i &1 &0 &\ldots &0 &0\\ 0 &\lambda_i &1 &0 &\ldots &0 \\ 0 &\vdots &\vdots &0 &\lambda_i &1 \\ 0 &\ldots &0 &0 &0 &\lambda_i \end{bmatrix}$.
\end{lemma}

\begin{lemma}\label{simtran}
Let $A\in \C^{\dcd}$ be asymptotically stable. There exists a matrix $U\in \gln$ such that $A=U\Lambda U^{-1}$ and $\Lambda^\dag+\Lambda$ is a real symmetric positive definite matrix.
\end{lemma}
\begin{proof}
It is trivial to see that for any $\Lambda\in \C^{\dcd}$, $\left(\Lambda^\dag+\Lambda\right)$ is real symmetric. We will use the decomposition of $A=V \tL V^{-1}$ in \Cref{jordon} and also carry over the notations in \Cref{jordon}. Consider the diagonal matrices $D^i_j=\begin{bmatrix} 1  &0 &0 &\ldots &0 &0\\ 0 &\re{\lambda_i} &0 &0 &\ldots &0 \\ 0 &\vdots &\vdots &0 &\re{\lambda}^{d^i_j-1}_i &0 \\ 0 &\ldots &0 &0 &0 &\re{\lambda}^{d^i_j}_i \end{bmatrix},\,\forall j=1,\ldots,l(i)$, $D^i=D^i_1 \op\ldots\op D^i_{l(i)},\,\forall i=1,\ldots,k$ and $D=D^1 \op\ldots\op D^k$.
It follows that $A=(VD) \Lambda (VD)^{-1}$, where $J$ is a matrix such that
$\Lambda=A_1 \op \ldots \op A_k$, where each $A_i,\,i=1,\ldots,k$ can further be written as
$A_i=\Lambda^i_{1} \op \ldots \op \Lambda^i_{{l(i)}}$. Each of $\Lambda^i_{j}$ is a $d^i_j\times d^i_j$ square matrix with the special form given by
$\Lambda^i_{j}=\begin{bmatrix} \lambda_i &\re{\lambda_i} &0 &\ldots &0 &0\\ 0 &\lambda_i &\re{\lambda_i} &0 &\ldots &0 \\ 0 &\vdots &\vdots &0 &\lambda_i &\re{\lambda_i} \\ 0 &\ldots &0 &0 &0 &\lambda_i \end{bmatrix}$.

Now let $\frac{(\Lambda^\dag+\Lambda)}{2}=\op_{i=1}^k \op_{j=1}^{l(i)}\frac{\Lambda^{i\dag}_{j}+\Lambda^i_{j}}{2}$, where $\frac{\Lambda^{i\dag}_{j}+\Lambda^i_{j}}{2}=\begin{bmatrix} \re{\lambda_i} &\frac{\re{\lambda_i}}{2} &0 &\ldots &0 &0\\ \frac{\re{\lambda_i}}{2} &\re{\lambda_i} &\frac{\re{\lambda_i}}{2} &0 &\ldots &0 \\ 0 &\vdots &\vdots &0 &\re{\lambda_i} &\frac{\re{\lambda_i}}{2} \\ 0 &\ldots &0 &0 &\frac{\re{\lambda_i}}{2} &\re{\lambda_i} \end{bmatrix} $. Then for any $x=(x(i),i=1,\ldots,n)\in \C^d (\neq \mathbf{0})$, there exists a $b\in \{-1,1\}^d$, such that
\begin{align*}
x^\dag \frac{(\Lambda^\dag+\Lambda)}{2} x &=\re{\lambda_i} \left(\sum_{i=1}^d \bar{x}{(i)} x(i)+\sum_{i=1}^{n-1} \frac{\bar{x}(i) x(i+1) + x(i)\bar{x}(i+1)}{2}\right) \\
&=\frac{\re{\lambda_i}}{2}\left(\md{x(1)}^2+ \md{x(n)}^2\right)+\frac{\re{\lambda_i}}{2}\left( \sum_{i=1}^{n-1} \md{x(i)}^2+\bar{x}(i) x(i+1) + x(i)\bar{x}(i+1)+\md{x(i+1)}^2 \right)\\
&>\frac{\re{\lambda_i}}{2}\left(\sum_{i=1}^d \md{x(i)+x(i+1)}^2 \right)\\
&> 0
\end{align*}
\end{proof}

\section{Error Recursion}


\begin{lemma}
Let $P$ be a positive definite distribution over $\C_B^{\dcd}$, then there exists an $\alpha_P$ such that $\rhod{P}>0$ and $\rhos{P}>0,~\forall \alpha \in (0,\alpha_P)$.
\end{lemma}
\begin{proof}
Since $P$ is positive definite from \Cref{distpd} it follows that $A_P$ is positive definite. Now
\begin{align*}
\rhos{P}&={\inf}_{x\in \C^d\colon\norm{x}=1}\ip{x,\left((A_P+A_P^\dag)-\alpha C_P\right)x}\\
&\geq {\inf}_{x\in \C^d\colon\norm{x}=1}\ip{x,\left((A_P+A_P^\dag)\right)x}-\alpha B^2\\
&\geq \lambda_{\min}(A_P+A_P^\dag)-\alpha B^2
\end{align*}
By choosing $\alpha_P<\frac{\lambda_{\min}(A_P+A_P^\dag)}{B^2}$, it follows that $\rhos{P}>0,\forall \alpha \in (0,\alpha_P)$. Further, by noting that $C_P\succeq A_P^\dag A_P$, it is easy to check that $\rhod{P}>\rhos{P}>0,\,\forall \alpha \in(0,\alpha_P)$.
\end{proof}


\begin{theorem}\label{pdrate}
Let $P$ in \Cref{lsadist} be a positive definite distribution over $\C_B^{\dcd}$. Then
\begin{align}
\EE{\norm{\thh_t-\theta^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P}}\right)\frac{1}{\alpha \rhos{P}} \left(\frac{\norm{\theta_0-\theta^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\theta_0-\theta^*})}{t+1} \right)\,.
\end{align}
\end{theorem}
\begin{proof}

\begin{align*}
\theta_t&=\theta_{t-1}+\alpha\big(b_t-A_t\theta_{t-1}\big)\\
\theta_t-\ts&=\theta_{t-1}-\ts+\alpha\big(b_t-A_t(\theta_{t-1}-\ts+\ts)\big)\\
e_t&=(I-\alpha A_t)e_{t-1}+(b_t -b -(A_t-A)\ts)\\
\end{align*}
Now define $\forall,\,i\geq j$, $F_{i,j}=(I-\alpha A_i)\ldots (I-\alpha A_j)$ and $\forall,\,i<j$ $F_{i,j}=\I$. Also let $\zeta_t\eqdef b_t-b-(A_t-A)\ts$ and $\EE{\norm{\zeta_t}^2}=\sigma_1^2=\sigma_P^2\norm{\ts}^2+\sigma_b^2$ and $\EE{\norm{M_t\zeta_t}}=\sigma_2^2=\sigma_P^2\norm{\ts}$.



\begin{align*}
e_t
& = (I-\alpha A_t) (I-\alpha A_{t-1}) e_{t-2}\\ &+ \alpha (I-\alpha A_t) \zeta_{t-1} +\alpha \zeta_t \\
& \quad \vdots\\
& = (I-\alpha A_t) \cdots (I-\alpha A_1) e_0\\ &+ \alpha (I-\alpha A_t) \cdots (I-\alpha A_2) \zeta_1 \\
& + \alpha (I-\alpha A_t) \cdots (I-\alpha A_3) \zeta_2\\
&  \quad \vdots \\
&+ \alpha \zeta_t\,,
\end{align*}
which can be written compactly as
\begin{align}
\label{eq:etft}
e_t = F_{t,1} e_0 + \alpha (F_{t,2} \zeta_1 + \dots + F_{t,t+1} \zeta_t )\,,
\end{align}
\begin{align*}
\eh_t=\frac{1}{t+1}{\sum}_{i=0}^{t}e_i
=\frac{1}{t+1}&\Big\{{\sum}_{i=0}^{t} F_{i,1} e_0 \\
&+ \alpha \sum_{i=1}^{t} \left(\sum_{k=i}^{t} F_{k,i+1} \right)\zeta_i \Big\} ,
\end{align*}
where in the second sum we flipped the order of sums and swapped the names of the variables that the sum runs over.
It follows that \todoc{We should rather use $C$ instead of $H$ here?}
\begin{align*}
\E[\norm{\eh_t}^2]&=\E\ip{\eh_t,\eh_t}
=\frac{1}{(t+1)^2} \sum_{i,j=0}^t \E\ip{e_i,e_j}\,.
\end{align*}
Hence, we see that it suffices to bound $\EE{\ip{ e_i,  e_j }}$.
There are two cases depending on whether $i=j$. When $i< j$,
\begin{align*}
\E\ip{e_i,e_j}
&=\E \ip{e_i,\big[F_{j,i+1} e_i+\alpha\textstyle\sum_{k=i+1}^j F_{j,k+1}\zeta_{k}\big]}\\
&=\E\ip{e_i,F_{j,i+1} e_i}  \text{(from \cref{noisecancel})}\\
&=\E\ip{e_i, (I-\alpha A)^{j-i} e_i} \text{(from \cref{lem:unroll})}
\end{align*}
and therefore
\begin{align*}
\label{inter}
\sum_{i=0}^{t-1}\sum_{j=i+1}^t \E\ip{e_i,e_j}
&=\frac1{\alpha\rhod{P}} {\sum}_{i=0}^{t-1}\E\ip{e_i,e_i}\\
&\leq \frac2{\alpha\rhod{P}}{\sum}_{i=0}^{t}\E\ip{e_i,e_i}\,.
\end{align*}
Since $\sum_{i,j}\cdot{} = \sum_{i=j}\cdot{} + 2 \sum_i \sum_{j>i} \cdot{}$,
\begin{align*}
{\sum}_{i=0}^{t}{\sum}_{j=0}^{t} \E\ip{e_i,e_j}&= \left(1+\frac2{\alpha\rhod{P}}\right){\sum}_{i=0}^{t}\E\ip{e_i,e_i}\,.
\end{align*}
Expanding $e_i$ using \eqref{eq:etft} and then using \cref{innerproduct} and \Cref{cmplxassmp}
\begin{align*}
\E\ip{e_i,e_i}&=\E\ip{F_{i,1}e_0,F_{i,1}e_0}+\alpha^2{\sum}_{j=1}^i\E\ip{ F_{i,j+1}\zeta_j, F_{i,j+1}\zeta_j}+\alpha\sum_{j=1}^i  \E\ip{F_{i,1} e_0, F_{i,j+1}\zeta_j}\\
&\leq (1-\alpha\rhos{P})^i\norm{e_0}^2+ \alpha^2\frac{{\sigma}_1^2}{\alpha \rhos{P}}+ \alpha \frac{{\sigma}^2_2 \norm{e_0}}{\alpha\rhos{P}}\,,
\end{align*}
and so
\begin{align*}
{\sum}_{i=0}^{t}{\sum}_{j=0}^{t} \E\ip{e_i,e_j}
&\leq \left(1+\frac2{\alpha\rhod{P} }\right)\, \frac1{\alpha\rhos{P}}\, (t(\alpha^2{\sigma}_1^2+\alpha {\sigma}^2_2\norm{e_0}) +\norm{e_0}^2)\,.
\end{align*}
Putting things together,
\begin{align}
\E[\norm{\eh_t}^2]
\leq \left(1+\frac2{\alpha\rhod{P}}\right)\, \frac1{\alpha\rhos{P}}\, \,
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{e_0}}{t+1} \right)\,.
\end{align}
\end{proof}

\begin{lemma}[Similarity Transformation]
Let $P^{AS}\in\P$ be asymptotically stable, then there exists a $U\in \gln$ such that $P^{AS}_U$ is positive definite.
\end{lemma}
\begin{proof}
Follows from \Cref{simtran}
\end{proof}


\begin{theorem}[Change of Basis]
Let $P$ be as in \Cref{lsadist}. Define $\gamma_t\eqdef U^{-1}\theta_t,\,,\gamma^*\eqdef U^{-1}\theta^*$, and suppose if $P_U$ is positive definite, then
\begin{align}
\EE{\norm{\gamma_t-\gamma^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U^{-1}}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\gamma_0-\gamma^*}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\gamma_0-\gamma^*})}{t+1} \right)\,,
\end{align}
where $\gh_t=\frac{1}{t+1}\sum_{s=0}^t \gamma_s$.
\end{theorem}
\begin{proof}
From proof of \Cref{pdrate} we know that
\begin{align*}
e_t&=(I-\alpha A_t)e_{t-1}+\zeta_t\\
U^{-1}e_t&=(I-\alpha U^{-1}A_t U) U^{-1}e_{t-1}+ U^{-1}\zeta_t\\
z_t&=(I-\alpha \Lambda_t) z_{t-1}+ H_t,
\end{align*}
where $z_t=\gamma_t-\gamma^*$, $\Lambda_t=U^{-1}A_t U$ and $H_t=U^{-1}\zeta_t$. we know that $\EE{\norm{H_t}^2}\leq \norm{U^{-1}}^2\EE{\norm{\zeta}}$ and $\EE{\norm{\Lambda_t H_t}}=\EE{\norm{U^{-1}A_t UU^-1\zeta_t}}=\EE{\norm{U^{-1}A_t \zeta_t}}\leq \norm{U^-1}\EE{\norm{A_t\zeta_t}}=\norm{U^{-1}}\sigma_2^2$. We follow through the steps of \Cref{pdrate} to obtain
\begin{align}
\E[\norm{\zh_t}^2]
&\leq \left(1+\frac2{\alpha\rhod{P_U}}\right)\, \frac1{\alpha\rhos{P_U}}\, \,
\left(\frac{\norm{z_0}^2}{(t+1)^2}+ \frac{\alpha^2(\norm{U^{-1}}^2\sigma_1^2)+\alpha (\norm{U^{-1}}\sigma_2^2)\norm{z_0}}{t+1} \right)\,\\
&\leq \left(1+\frac2{\alpha\rhod{P_U}}\right)\, \frac1{\alpha\rhos{P_U}}\, \,
\left(\frac{\norm{U^{-1}}^2\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2(\norm{U^{-1}}^2\sigma_1^2)+\alpha (\norm{U^{-1}}\sigma_2^2)\norm{U^{-1}}\norm{e_0}}{t+1} \right)\,
\end{align}

\end{proof}


Let $\F_t = \sigma( A_1,\dots,A_t, \zeta_1,\dots,\zeta_t ) $, with $\F_0$ the $\sigma$-field that holds all random variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Product unroll lemma]\label{lem:genunroll}
Let $t>i\ge 1$, $x,y\in \C^d$ be $\F_{i}$-measurable random vectors. Then,
\begin{align*}
\E[x^\dag F_{t,i+1}y|\F_i]=x^\dag (I-\alpha H)^{t-i} y\,.
\end{align*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By the definition of $F_{t,i+1}$,
and because $F_{t-1,i+1} = (I-\alpha A_{t-1}) \dots (I-\alpha A_{i+1})$ is $\F_{t-1}$-measurable,
as are $x$ and $y$,
\begin{align*}
\EE{x^\dag F_{t,i+1} y | \F_{t-1} } &= x^\top \EE{ (I-\alpha A_t) | \F_{t-1} } F_{t-1,i+1} y\\
&=x^\dag  (I-\alpha A_P)  F_{t-1,i+1} y\,.
\end{align*}
By the tower-rule for conditional expectations and our measurability assumptions,
\begin{align*}
\EE{x^\dag F_{t,i+1} y | \F_{t-2} }
&=x^\dag  (I-\alpha A_P)  \EE{F_{t-1,i+1} |\F_{t-2}} y\\
&= x^\dag (I-\alpha A_P)^2 F_{t-2,i+1} y\,.
\end{align*}
Continuing this way we get
\begin{align*}
\EE{x^\dag F_{t,i+1} y | \F_{t-j} }
= x^\dag (I-\alpha A_P)^j F_{t-j,i+1} y\,, \quad j=1,2,\dots,t-i\,.
\end{align*}
Specifically, for $j=t-i$ we get
\begin{align*}
\EE{x^\dag F_{t,i+1} y | \F_{i} }  = x^\dag (I-\alpha A_P)^{t-i} y\,.
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{noisecancel}
Let $t>i\ge 1$ and let $x$ be an $\F_{i-1}$-measurable random vector. Then,
$\E[x^\dag F_{t,i+1}\zeta_{i}]=0$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By \cref{lem:genunroll},
\begin{align*}
\EE{x^\dag F_{t,i+1} \zeta_i | \F_{i} }  = x^\dag (I-\alpha A_P)^{t-i} \zeta_i\,.
\end{align*}
Using the tower rule,
\begin{align*}
\EE{x^\dag F_{t,i+1} \zeta_i | \F_{i-1} }
= x^\dag (I-\alpha A_P)^{t-i}\EE{ \zeta_i | \F_{i-1} }= 0\,.
\end{align*}
\end{proof}

\begin{lemma}\label{lem:unroll}
For all $t>i\ge 0$, $\E \ip{e_i,F_{t,i+1} e_i}=\E\ip{e_i,(I-\alpha A_P)^{t-i} e_i}$.
\end{lemma}
\begin{proof}
The lemma follows directly from \cref{lem:genunroll}. Indeed,
$\theta_i$ depends only on $A_1,\dots,A_{i},b_1,\dots,b_{i}$, $\theta_i$ and so is $e_i$ $\F_i$-measurable.
Hence, the lemma is applicable and implies that
\begin{align*}
\EE{ \ip{e_i, F_{t,i+1} e_i} | \F_i } =
\EE{ \ip{e_i, (I-\alpha A_P)^{t-i} e_i} | \F_i }\,.
\end{align*}
Taking expectation of both sides gives the desired result.
\end{proof}

\begin{lemma}\label{innerproduct}
Let $i>j \ge 0$ and let $x\in \R^d$ be an $\F_j$-measurable random vector.
Then,
\begin{align*}
\E\ip{F_{i,j+1}x,F_{i,j+1}x}\leq (1-\alpha \rho_\alpha)^{i-j}\E\norm{x}^2\,.
\end{align*}
\end{lemma}
\begin{proof}
Note that
$S_t\doteq \EE{ (I-\alpha A_t)^\dag (I-\alpha A_t) | \F_{t-1} }
= I - \alpha (A_P^\dag + A_P) + \alpha^2 \EE{ A_t^\dag A_t | \F_{t-1} }$.
Since $(b_t,A_t)_t$ is an independent sequence, $\EE{ A_t^\dag A_t|\F_{t-1}} = \EE{ A_1^\dag A_1 }$.
Now, using the definition of $\rhos{P}$ from \eqref{contract},
$\sup_{x:\norm{x}= 1} x^\top S_t x = 1 - \alpha \inf_{x:\norm{x}=1} x^\top (A_P^\dag + A_P - \alpha \EE{A_1^\top A_1}) x
= 1-\alpha \rhos{P}$.
Hence,
\begin{align*}
&\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-1} }\\
&= \EE{x^\dag F_{i-1,j+1}^\top (I-\alpha A_i)^\dag (I-\alpha A_i) F_{i-1,j+1} x\,|\,\F_{i-1}}\\
&=(x F_{i-1,j+1})^\dag \, S_i \, F_{i-1,j+1} x\\
&\le (1-\alpha \rhos{P}) \, \ip{ F_{i-1,j+1} x, F_{i-1,j+1} x} \\
& \le (1-\alpha \rhos{P})^2\, \ip{ F_{i-2,j+1} x, F_{i-2,j+1} x} \\
& \quad \vdots \\
& \le (1-\alpha \rho_\alpha)^{i-j}\, \norm{x}^2\,.
%&= \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_i)^\top (I-\alpha H_i)\ldots (I-\alpha H_{j+1})x|\F_{i-1}]\\
%&=(1-\alpha\rhos{P}) \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_{i-1})^\top (I-\alpha H_{i-1})\ldots (I-\alpha H_{j+1})x]
\end{align*}
\end{proof}


\begin{comment}
Let $\F_t = \sigma( H_1,\dots,H_t, \zeta_1,\dots,\zeta_t ) $, with $\F_0$ the $\sigma$-field that holds all random variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Product unroll lemma]\label{lem:genunroll}
Let $t>i\ge 1$, $x,y\in \R^d$ be $\F_{i}$-measurable random vectors. Then,
\begin{align*}
\E[x^\top F_{t,i+1}y|\F_i]=x^\top (I-\alpha H)^{t-i} y\,.
\end{align*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By the definition of $F_{t,i+1}$,
and because $F_{t-1,i+1} = (I-\alpha H_{t-1}) \dots (I-\alpha H_{i+1})$ is $\F_{t-1}$-measurable,
as are $x$ and $y$,
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{t-1} } &= x^\top \EE{ (I-\alpha H_t) | \F_{t-1} } F_{t-1,i+1} y\\
&=x^\top  (I-\alpha H)  F_{t-1,i+1} y\,.
\end{align*}
By the tower-rule for conditional expectations and our measurability assumptions,
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{t-2} }
&=x^\top  (I-\alpha H)  \EE{F_{t-1,i+1} |\F_{t-2}} y\\
&= x^\top (I-\alpha H)^2 F_{t-2,i+1} y\,.
\end{align*}
Continuing this way we get
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{t-j} }
= x^\top (I-\alpha H)^j F_{t-j,i+1} y\,, \quad j=1,2,\dots,t-i\,.
\end{align*}
Specifically, for $j=t-i$ we get
\begin{align*}
\EE{x^\top F_{t,i+1} y | \F_{i} }  = x^\top (I-\alpha H)^{t-i} y\,.
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{noisecancel}
Let $t>i\ge 1$ and let $x$ be an $\F_{i-1}$-measurable random vector. Then,
$\E[x^\top F_{t,i+1}\zeta_{i}]=0$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By \cref{lem:genunroll},
\begin{align*}
\EE{x^\top F_{t,i+1} \zeta_i | \F_{i} }  = x^\top (I-\alpha H)^{t-i} \zeta_i\,.
\end{align*}
Using the tower rule,
\begin{align*}
\EE{x^\top F_{t,i+1} \zeta_i | \F_{i-1} }
= x^\top (I-\alpha H)^{t-i}\EE{ \zeta_i | \F_{i-1} }= 0\,.
\end{align*}
\end{proof}

\begin{lemma}\label{lem:unroll}
For all $t>i\ge 0$, $\E \ip{He_i,H F_{t,i+1} e_i}=\E\ip{He_i,H(I-\alpha H)^{t-i} e_i}$.
\end{lemma}
\begin{proof}
The lemma follows directly from \cref{lem:genunroll}. Indeed,
$\theta_i$ depends only on $H_1,\dots,H_{i},g_1,\dots,g_{i}$, $\theta_i$ and so is $e_i$ $\F_i$-measurable.
Hence, the lemma is applicable and implies that
\begin{align*}
\EE{ \ip{He_i, HF_{t,i+1} e_i} | \F_i } =
\EE{ \ip{He_i, H (I-\alpha H)^{t-i} e_i} | \F_i }\,.
\end{align*}
Taking expectation of both sides gives the desired result.
\end{proof}

\begin{lemma}\label{innerproduct}
Let $i>j \ge 0$ and let $x\in \R^d$ be an $\F_j$-measurable random vector.
Then,
\begin{align*}
\E\ip{F_{i,j+1}x,F_{i,j+1}x}\leq (1-\alpha \rho_\alpha)^{i-j}\E\norm{x}^2\,.
\end{align*}
\end{lemma}
\begin{proof}
Note that
$S_t\doteq \EE{ (I-\alpha H_t)^\top (I-\alpha H_t) | \F_{t-1} }
= I - \alpha (H^\top + H) + \alpha^2 \EE{ H_t^\top H_t | \F_{t-1} }$.
Since $(g_t,H_t)_t$ is an independent sequence, $\EE{ H_t^\top H_t|\F_{t-1}} = \EE{ H_1^\top H_1 }$.
Now, using the definition of $\rho_\alpha$ from \eqref{eq:rhodef},
$\sup_{x:\norm{x}= 1} x^\top S_t x = 1 - \alpha \inf_{x:\norm{x}=1} x^\top (H^\top + H - \alpha \EE{H_1^\top H_1}) x
= 1-\alpha \rho_\alpha$.
Hence,
\begin{align*}
&\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-1} }\\
&= \EE{x^\top F_{i-1,j+1}^\top (I-\alpha H_i)^\top (I-\alpha H_i) F_{i-1,j+1} x\,|\,\F_{i-1}}\\
&=(x F_{i-1,j+1})^\top \, S_i \, F_{i-1,j+1} x\\
&\le (1-\alpha \rho_\alpha) \, \ip{ F_{i-1,j+1} x, F_{i-1,j+1} x} \\
& \le (1-\alpha \rho_\alpha)^2\, \ip{ F_{i-2,j+1} x, F_{i-2,j+1} x} \\
& \quad \vdots \\
& \le (1-\alpha \rho_\alpha)^{i-j}\, \norm{x}^2\,.
%&= \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_i)^\top (I-\alpha H_i)\ldots (I-\alpha H_{j+1})x|\F_{i-1}]\\
%&=(1-\alpha\rho_{\alpha}) \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_{i-1})^\top (I-\alpha H_{i-1})\ldots (I-\alpha H_{j+1})x]
\end{align*}
\end{proof}



\begin{align*}
U^{-1}e_t&=(I-\alpha U^{-1}A_t U) U^{-1}e_{t-1}+ U^{-1}(b_t -b -(A_t-A)\ts)\\
e_t&=(I-\alpha J_t) e_{t-1}+ H_t
\end{align*}

\begin{align*}
e_t
& = (I-\alpha J_t) (I-\alpha J_{t-1}) e_{t-2}\\ &+ \alpha (I-\alpha J_t) \zeta_{t-1} +\alpha H_t \\
& \quad \vdots\\
& = (I-\alpha J_t) \cdots (I-\alpha J_1) e_0\\ &+ \alpha (I-\alpha J_t) \cdots (I-\alpha J_2) \zeta_1 \\
& + \alpha (I-\alpha J_t) \cdots (I-\alpha J_3) \zeta_2\\
&  \quad \vdots \\
&+ \alpha H_t\,,
\end{align*}
which can be written compactly as
\begin{align}
\label{eq:etft}
e_t = F_{t,1} e_0 + \alpha (F_{t,2} \zeta_1 + \dots + F_{t,t+1} H_t )\,,
\end{align}
\begin{align*}
\zh_t=\frac{1}{t+1}{\sum}_{i=0}^{t}e_i
=\frac{1}{t+1}&\Big\{{\sum}_{i=0}^{t} F_{i,1} e_0 \\
&+ \alpha \sum_{i=1}^{t} \left(\sum_{k=i}^{t} F_{k,i+1} \right)\zeta_i \Big\} ,
\end{align*}
where in the second sum we flipped the order of sums and swapped the names of the variables that the sum runs over.

It follows that \todoc{We should rather use $C$ instead of $H$ here?}
\begin{align*}
\E[\norm{\zh_t}^2]&=\E\ip{\zh_t,\zh_t}
=\frac{1}{(t+1)^2} \sum_{i,j=0}^t \E\ip{e_i,e_j}\,.
\end{align*}
Hence, we see that it suffices to bound $\EE{\ip{ e_i,  e_j }}$.
There are two cases depending on whether $i=j$. When $i< j$,
\begin{align*}
\E\ip{e_i,e_j}
&=\E \ip{e_i,\big[F_{j,i+1} e_i+\alpha\textstyle\sum_{k=i+1}^j F_{j,k+1}\zeta_{k}\big]}\\
&=\E\ip{e_i,F_{j,i+1} e_i}  \text{(from \cref{noisecancel})}\\
&=\E\ip{e_i, (I-\alpha J)^{j-i} e_i} \text{(from \cref{lem:unroll})}
\end{align*}
and therefore
\begin{align*}
\label{inter}
\sum_{i=0}^{t-1}\sum_{j=i+1}^t \E\ip{e_i,e_j}
&=\frac1{\alpha\rhod{P}} {\sum}_{i=0}^{t-1}\E\ip{e_i,e_i}\\
&\leq \frac2{\alpha\rhod{P}}{\sum}_{i=0}^{t}\E\ip{e_i,e_i}\,.
\end{align*}
Since $\sum_{i,j}\cdot{} = \sum_{i=j}\cdot{} + 2 \sum_i \sum_{j>i} \cdot{}$,
\begin{align*}
{\sum}_{i=0}^{t}{\sum}_{j=0}^{t} \E\ip{e_i,e_j}&= \left(1+\frac2{\alpha\rhod{P}}\right){\sum}_{i=0}^{t}\E\ip{e_i,e_i}\,.
\end{align*}
Expanding $e_i$ using \eqref{eq:etft} and then using \cref{innerproduct} and \Cref{cmplxassmp}
\begin{align*}
\E\ip{e_i,e_i}&=\E\ip{F_{i,1}e_0,F_{i,1}e_0}+\alpha^2{\sum}_{j=1}^i\E\ip{ F_{i,j+1}\zeta_j, F_{i,j+1}\zeta_j}+\alpha\sum_{j=1}^i  \E\ip{F_{i,1} e_0, F_{i,j+1}\zeta_j}\\
&\leq (1-\alpha\rhos{P})^i\norm{e_0}^2+ \alpha^2\frac{{\sigma}_1^2}{\alpha \rhos{P}}+ \alpha \frac{{\sigma}^2_2 \norm{e_0}}{\alpha\rhos{P}}\,,
\end{align*}
and so
\begin{align*}
{\sum}_{i=0}^{t}{\sum}_{j=0}^{t} \E\ip{e_i,e_j}
&\leq \left(1+\frac2{\alpha\rhod{P} }\right)\, \frac1{\alpha\rhos{P}}\, (t(\alpha^2{\sigma}_1^2+\alpha {\sigma}^2_2\norm{e_0}) +\norm{e_0}^2)\,.
\end{align*}
Putting things together,
\begin{align}
\E[\norm{\zh_t}^2]
\leq \left(1+\frac2{\alpha\rhod{P}}\right)\, \frac1{\alpha\rhos{P}}\, \,
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+\alpha {\sigma}^2_2\norm{e_0}}{t+1} \right)\,.
\end{align}

\end{comment}

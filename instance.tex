\section{Main Results}\label{sec:mainresults}
In this section, we derive instance dependent bounds  which is valid for a fixed $P$ and in the \Cref{sec:uniform}, address the question of deriving uniform bounds $\forall\,P\in \P$, where $\P$ is a class of distributions over $\C^{\dcd}$. We first present the results and then follow it up with a discussion.
\begin{lemma}\label{lm:pd}
Let $P$ be a positive definite distribution over $\C^{\dcd}$, then there exists an $\alpha_P$ such that $\rhod{P}>0$ and $\rhos{P}>0,~\forall \alpha \in (0,\alpha_P)$.
\end{lemma}
\begin{comment}
\begin{proof}
\begin{align*}
\rhos{P}&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^*\EE{A_t^* A_t} x\\
&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^* A^*_P A_P -\alpha x^* \EE{M_t^* M_t} x\\
&\geq \lambda_{\min}(A^*_P+A_P)-\alpha \norm{A_P}^2-\sigma^2_P
\end{align*}
The proof is complete by choosing $\alpha_P<\frac{\lambda_{\min}(A^*_P+A_P)}{\norm{A_P}^2+\sigma^2_P}$
\end{proof}
\end{comment}
\begin{lemma}\label{lm:simtran}[Similarity Transformation]
Let $P^{AS}$ be asymptotically stable, then there exists a $U\in \gln$ such that $P^{AS}_U$ is positive definite.
\end{lemma}

\begin{theorem}\label{th:pdrate}
Let $P$ in \Cref{assmp:lsa}-~\ref{dist} be a positive definite distribution over $\C^{\dcd}$. Then
\begin{align*}
\EE{\norm{\thh_t-\ts}^2}
\leq
\left(1+\frac2{\alpha\rhod{P}}\right)\frac{1}{\alpha \rhos{P}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\theta_0-\ts})}{t+1} \right)\,.
\end{align*}

\end{theorem}
\begin{theorem}\label{thm:simtran}[Change of Basis]
Let $P$ in \Cref{assmp:lsa}-~\ref{dist} be AS. Define $\gamma_t\eqdef U^{-1}\theta_t,\,,\gamma^*\eqdef U^{-1}\ts$, and suppose if $P_U$ is positive definite, then
\begin{align*}
\EE{\norm{\gh_t-\gamma^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U^{-1}}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\gamma_0-\gamma^*}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\gamma_0-\gamma^*})}{t+1} \right)\,,
\end{align*}
where $\gh_t=\frac{1}{t+1}\sum_{s=0}^t \gamma_s$.
\end{theorem}
\begin{corollary}\label{cor:as}
Let $P$ in \Cref{lsadist} be asymptotically stable. Then, there exists
\begin{align*}
\EE{\norm{\thh_t-\ts}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\cond{U}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
\end{align*}
\end{corollary}
\begin{theorem}[Lower Bound]
There exists a $P$ such that
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}

\end{theorem}
\begin{comment}
Consider the LSA with $b_t=0$, and $P^0$ has all the mass concentrated on $A_{P^0}=\begin{bmatrix} \lambda_{\min} &0\\ 0& \lambda_{\max}\end{bmatrix}$, for some $\lambda_{\max}>\lambda_{\min}>0$. Note that in this example $\ts=0$.
By choosing $\alpha<\frac2{\lambda_{\max}}$, in this case it is straightforward to write the expression for $\eh_t$ explicitly as below:
\begin{align*}
\eh_t&=\frac{1}{t+1}\sum_{s=0}^t e_t = \frac{1}{t+1}\sum_{s=0}^t (I-\alpha A_P)^{t-s} e_0\\
&=\frac{1}{t+1}(\alpha A)^{-1}\left(I-(I-\alpha A)^{t+1}\right)e_0
\end{align*}

\begin{align*}
\norm{\eh_t}^2&=\frac{1}{(t+1)^2}\norm{(\alpha A_P)^{-1}\left(I-(I-\alpha A_P)^{t+1}\right)e_0}^2
&\leq\frac{1}{(t+1)^2}\alpha^2\norm{A_P^{-1}}^2\norm{e_0}^2
\end{align*}

\begin{align*}
\norm{\eh_t}^2\geq \eh^2_t(1)=\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)
\end{align*}
Note that in this example, $\rhos{P^0}=\rhod{P^0}=\lambda_{\min} -\alpha \lambda_{\min}^2=\lambda_{\min}(1-\alpha \lambda_{\min})$. Sandwiching the error between the lower and upper bounds we have
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}
\end{comment}

\subsection{Result Discussion}
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item \textbf{Role of $\rhos{P}$ and $\rhod{P}$:} We can expand the MSE as \begin{align}\label{eq:exp}\EE{\norm{\eh_t}}=\frac{1}{(t+1)^2}\ip{\sum_{s=0}^t e_s,\sum_{s=0}^t e_s}.\end{align} The inner product in \eqref{eq:exp} is a summation of \emph{diagonal} terms $\EE{\ip{e_i,e_i}}$ and \emph{corss} terms of$\EE{\ip{e_i,e_j}},i\neq j$. The growth of the diagonal terms and the cross terms depends on the spectral norm of the matrices $I-\alpha A_t$ and $I-\alpha A_P$ respectively. These are given by
\begin{comment}
The output $\thh_t$ of the algorithm \eqref{eq:lsa} is the average of the internal states at times $s=0,\ldots,t-1$. The error dynamics of the internal states by looking at the behavior of $e_t\eqdef \theta_t-\ts$.
\begin{align}\label{eq:errec}
\theta_t&=\theta_{t-1}+\alpha(b_t-A_t\theta_{t-1})\nn\\
\theta_t-\ts&=\theta_{t-1}-\ts+\alpha\left(b_t-A_t(\theta_{t-1}-\ts+\ts)\right)\nn\\
e_t&=e_{t-1}+\alpha(b_t-A_t e_t -A_t\ts)\nn\\
e_t&=\underbrace{((I-\alpha A_t))}_{\text{Random-Matrix}} e_{t-1}+\underbrace{\alpha(N_t -(M_t)\ts)}_{\text{Noise}}
\end{align}
From \eqref{errec} it is clear that the error dynamics depends on the $i)$ properties of the random matrix, $ii)$ properties of the noise. In the absence of the noise term in \eqref{eq:errec}, we have $e_t=(I-\alpha A_t) e_{t-1}=\Pi_{s=1}(I-\alpha A_s) e_0$, i.e., the inital error is get multiplied by a product of random matrices. In such a scenario, we can guess that whether or not the algorithm forgets the bias $\norm{\theta_0-\ts}$, depends on whether the matrix product is contracting. One way to characterize this contracting property is to look at the spectral radius of the random matrix, which is given by
\end{comment}
\begin{align}\label{eq:spectralrand}
\EE{\norm{I-\alpha A_t}^2}=1-\alpha \rhos{P}, {\norm{I-\alpha A_P}^2}=1-\alpha \rhod{P}
\end{align}
Note that \eqref{eq:spectralrand} explicitly connects $\rhod{P}$ and $\rhos{P}$ in \Cref{def:contract} to the spectral norms of the aforementioned matrices. For the MSE to be bounded, we need the spectral norms to be less than unity, which in turn translates to the requirement that $\rhos{P}>0$ and $\rhod{P}>0$.
\item \textbf{Role of $U$:} When $A_P$ is positive definite, it is straighforward to produce an $\alpha>0$ such that $\rhos{P}>0$ and $\rhod{P}>0$. However, $A_P$ is Hurwitz does not guarantee positive definiteness (see \Cref{ex:pdas}) and a result an $\alpha>0$ such that $\rhos{P}>0$ and $\rhod{P}>0$. This poses a difficulty in applying results of \Cref{th:pdrate} in the case Hurwitz matrices. This can be remedied by a similiarity transformation using an appropriate $U$ that transforms $A_P$ to appropriate Jordon form $A_{P_U}$. \Cref{th:simtran} presents the rates for the transformed iterates   $\gamma_t=U^{-1 }\theta_t$. Note that \Cref{cor:as} is obtained from \Cref{th:simtran} by the fact that $\theta_t=U\gamma_t$.
\item \textbf{Bias and Variance:} The means-squared error at $t$ is bounded by a sum of two terms. The first term is the bias term given by $\B=\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\cond{U}^2}{\alpha \rhos{P_U}}\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}$.  The \emph{bias} term that captures the rate at which the initial condition $\norm{\theta_0-\ts}^2$ is forgotten. The second term is the \emph{variance} given by $\V=\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\cond{U}^2}{\alpha \rhos{P_U}} \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} $. The variance term that captures the rate at which noise is rejected.
\item \textbf{$\alpha$, $\rhos{P}$ and $\rhod{P}$} only affect the constants in the rates and not the exponents $\frac{1}{t}$ for variance and $\frac{1}{t^2}$ for bias terms.
\item \textbf{Behaviour for extreme value of $\alpha$:} For smaller values of stepsize, i.e., $\alpha\approx 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ term. This is due to the fact that the stepsizes determine the learning rate and for smaller stepsizes the learning rate is slower. However, in this case the noise term does not blow up, a fact that can appreciated by looking at \Cref{th:pdrate,th:simtran,cor:as} where $\alpha$ is seen to multiply the variances $\sigma^2_P$ and $\sigma^2_b$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms can each other. For larger values of $\alpha$ i.e., $\alpha\ra \alpha_{P}$ (or $\alpha_{P_U}$ as the case may be), the bounds blow up again, due to the fact that $\rhos{P}\ra 0$ in this case.

\subsection{Illustrative Examples}
\begin{example}\label{ex:pdas}
Matrices $A_1=\begin{bmatrix} 2 &1 \\-1 &0\end{bmatrix}$$A_2=\begin{bmatrix}1 & -1\\ 1 &0 \end{bmatrix}$ and $A_3=\begin{bmatrix} 1 & 10-\epsilon \\ 0 &1 \end{bmatrix}$ are not positive definite. However choosing $U_1=\begin{bmatrix} \frac{1}{\sqrt{2}}& -\frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \end{bmatrix}$, $U_2=\begin{bmatrix} \frac{\sqrt{2}}{1-i\sqrt{3}} & \frac{\sqrt{2}}{1+i\sqrt{3}} \\ -\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}}\end{bmatrix}$ and $U_3=\begin{bmatrix} 1 & 0\\ 0 &\frac{1}{10} \end{bmatrix}$, we can see that matrices $J_i=U_i^{-1}A_i U_i,i=1,2,3$ are positive definite.
\end{example}
\paragraph{Remark}

\begin{example}\label{ex:geo}
Consider the one dimensional case where $A_t=\rho$ for some $0<\rho<1$, $b_t$ a bounded $i.i.d$ zero mean random variable with variance $\sigma^2$, and $\ts=0$. We know that $e_t=(1-\alpha\rho)^t e_0+\sum_{s=1}^t (1-\alpha \rho)^{t-s}\alpha b_s$ and the MSE is given by
\begin{align*}
\EE{\norm{\theta_t}^2}&=\frac{1}{(t+1)^2}\left( (\alpha\rho)^{-1}(1-\alpha\rho)^{t+1}) \theta_0+ \sum_{s=1}^t (\alpha\rho)^{-1}(1-\alpha \rho)^{t-s+1} \alpha b_s\right)^2\\
&\leq (\alpha\rho)^{-2}\left(\frac{\norm{\theta_0}^2}{t^2}+\frac{\alpha^2\sigma^2}{t}\right)
\end{align*}
\end{example}
\paragraph{Geometric Series:} In simple terms the error dynamics is akin to sum of a geometric series with common factor $\alpha\rho$, a fact that is illustrated by the simple $1$-dimensional example in \Cref{ex:geo}.
\begin{example}
Consider distributions $P^1$, $P^2$ and $P^3$ which are concentrated on matrices $A_{P^1}=\begin{bmatrix} 10 &100\\ 0 &10\end{bmatrix}$, $A_{P^2}=\begin{bmatrix} 10 &10\\ 0 &10\end{bmatrix}$, $A_{P^3}=\begin{bmatrix} 10 &0 \\ 0 &10\end{bmatrix}$. Note that $P^1$, $P^2$ and $P^3$ are respectively AS, PD, SPD respectively. Consider the LSA for the three case of $P^1$, $P^2$ and $P^3$ with $b_t=0$ and $\ts=0$
We now list the following observations:
\begin{itemize}%[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item In all the three instance, $0<\alpha<\frac2{10}$ ensures that the error $\norm{\eh_t}\ra 0$ as $t\ra\infty$. Thus the range of $\alpha$ is uniform across $P^i,i=1,2,3$.
\item $\rhos{P^1}=\rhod{P^1}<0$, and hence one cannot apply results of \Cref{th:pdrate}. However, with  $U=\begin{bmatrix}1 &0 \\ 0 &0.2-\epsilon\end{bmatrix}$ for some $\epsilon>0$ we have $P^1_U$ to be the distribution concentrated on the matrix $A_{P^1_U}=\begin{bmatrix} 10 &20 \\ 0 &10\end{bmatrix}$ and one can apply the results of \Cref{thm:simtran,cor:as}. Notice that the condition number $\norm{U}{U^-1}\approx 5$.
\end{itemize}
We plot $L_t$ for the three cases for $t=1,\ldots,100$ and we can see that the condition number $\norm{U}{U^-1}\approx 5$, shows up in the AS case ($P^1$).
\end{example}
\end{itemize}
\subsection{Related Work}
LSA with CS-PR was considered in \cite{polyak-judisky} under the additive noise assumption, wherein $A_t=A$. Recent work by \cite{bach} considers a stochastic gradient descent (SGD) algorithm for the problem of linear prediction given in \Cref{ex:linreg}.
\begin{example}\label{ex:linreg}
Let $(x_t,y_t)\in \R^d\times \R,\,t\geq 1$ be $i.i.d$ such that $\EE{\norm{x_t}^2}$ and $\EE{y_t^2}$ are finite. Further, we assume bounded data i.e., $\norm{(x_t,y_t)}^2\leq B$ for some positive real number $B>0$. Here, $x_t,\,t\geq 1$ are the features and $y_t$ are real labels and the linear regression problem constitutes minimizing the quadratic loss function $f(\theta)=\EE{\left(\ip{x_t,\theta}-y_t\right)^2}$. SGD with CS-PR to minimize $f(\theta)$ can be given as below:
\begin{subequations}\label{eq:linreg}
\begin{align}
\theta_t&=\theta_{t-1}-\alpha x_t\left(\ip{x_t,\theta_{t-1}} -y_t\right)\\
\thh_t&=\frac{1}{t+1}\sum_{s=0}^t \theta_t
\end{align}
\end{subequations}
Here $\nabla f(\theta_{t-1})=x_t\left(\ip{x_t,\theta_{t-1} -y_t}\right)$ is the \emph{stochastic gradient} of the objective $f(\theta)$. It follows from the $i.i.d$ assumption on $(x_t,y_t)$ that $A_t=x_t x_t^\top$ and $b_t=y_t x_t$ are also $i.i.d$, and with this identification one can see that \eqref{linreg} is of the form in \eqref{conststep}. In this case $A_P$ is SPD.
In the linear regression setting, the error of a parameter $\theta$ is measured as the difference between the loss at the said parameter and the loss at the optimum, which has the following form
\begin{align}\label{eq:mselinreg}
f(\theta)-f(\ts)=(\theta-\ts)^\top A_P (\theta-\ts)=\EE{\norm{\theta-\ts}^2_{A_P}}
\end{align}
\end{example}
In this setting, \cite{bach} show that the variance term in the MSE is at most $O(\frac{C}{t})$, where $C$ does not depend on the condition number. This ``remarkable'' result is due to the following facts: $(i)$ $A_P$ in this case is symmetric positive definite, $(ii)$ the MSE in \eqref{eq:mselinreg} is measured with respect to $A_P$, $(iii)$ the noise $\epsilon_t=y_t-\ip{\ts,x_{t-1}}$ is structured i.e., $\EE{\epsilon_t^2 x_t x_t^\top}\leq R A_P$ for some constant $R>0$.
At the same time, the bias term decays at a rate $\frac{C'}{t^2}$, where the constant $C'$ contains the factor $(\theta_0-\ts)^\top A^{-1}_P (\theta_0-\ts)$ and is dependent on the condition number.

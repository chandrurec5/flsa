\documentclass{article}
\input{pack}
% if you need to pass options to natbib, use, e.g.:
 \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{varioref}
%\usepackage{thmtools}
\usepackage{aliascnt}
\newcommand{\tcite}[1]{\citeauthor{#1}~\citeyear{#1}}
\usepackage{titlecaps}
\usepackage[capitalize]{cleveref}
\usepackage{amsmath,amssymb,graphicx}
\crefname{assumption}{Assumption}{Assumption}

% BIBLIOGRAHY -----------------------------------------------------------------------------
% Option 1: bibtex natbib
%\usepackage[numbers]{natbib}
%\usepackage[colorlinks=true,linkcolor=blue,citecolor=purple,pagebackref=true]{hyperref}
% Option 2: biblatex
%\usepackage[backend=bibtex,natbib=true,backref=false]{biblatex}
%\usepackage[backend=bibtex,style=ieee,natbib=true,backref=false]{biblatex}
%\addbibresource{ref.bib}
% Adding back references to bib entries:
%\DefineBibliographyStrings{english}{%
%backrefpage = {page},% originally "cited on page"
%backrefpages = {pages},% originally "cited on pages"
%}
% Title, authors, etc. -------------------------------------------------------------------------



\title{Issues with Fast LSTD using stochastic approximation: Finite time analysis and application to traffic control}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
%  David S.~Hippocampus\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
In what follows we describe the issues with \citet{flstd}. The complete proofs of \cite{flstd} are available in the arxiv version in \cite{flstda}.

\section{Problems with the Proof}

\subsection{Expected Error Bound}

The proof of Theorem 1 can be found in section A.2, starting on page 16 of \cite{flstda}.
The proof up to Eq. (26) is correct (in the definition of $M_{n+1}$, $F_n$ should be $F$).
However, after this display, we are told that by (A4), which ensures that $\frac1T \sum_{t=1}^T \phi_t \phi_t^\top \succ \mu I$ for some $\mu>0$, $\bar A_n = \frac1n \sum_{i=1}^n \phi_i (\phi_i - \beta \phi_i')^\top$ is such that $\bar A_n - (1-\beta) \mu I $ is positive definite. 
Here, $n>0$ is an arbitrary index and we use the abbreviation $\phi_t = \phi(s_{i_t})$ and $\phi_t' = \phi(s_{i_t}')$.
\begin{quote}
In general, (A4) does \emph{not} imply that  $\bar A_n - (1-\beta) \mu I $ is positive definite!
\end{quote}
Take for example $n=1$. We would need that $\phi_1 (\phi_1 - \beta \phi_1')^\top - (1-\beta) \mu I$ is positive definite.
It is easy to construct examples where this is not true. (Note that the matrices involved are \emph{not} symmetric. Unfortunately, none of the two papers defines what is meant by positive definite in this case. We assume that the definition used is that a square, real-valued matrix $A$ is called positive definite if $x^\top A x\ge 0$ for any $x$ real-valued vector of appropriate dimension.)

In fact, we don't see whether this holds even when $n=T$ and we in fact suspect that this claim is false in full generality.

\section{Problems with the Proof: High Probability Bound}

The proof of the high probability bound starts on page 14, in section A.1.
The first problem happens in the display on the bottom of this page in the proof of Lemma 6.
Here, we are told that for $A = \phi \phi^\top - \beta \phi (\phi')^\top$ (we are dropping indices to remove clutter),
\begin{align*}
A^\top A = \norm{\phi}^2 \phi \phi^\top - \beta ( 2 - \norm{\phi}^2\beta ) \phi' (\phi')^\top\,,
\end{align*}
where $\norm{x}$ denotes the $2$-norm of $x$.
However, using $A = \phi( \phi-\beta \phi')^\top$, a direct calculation gives:
\begin{align*}
A^\top A 
& = ( \phi-\beta \phi') \phi^\top \phi ( \phi-\beta \phi')^\top \\
& = \norm{\phi}^2  ( \phi-\beta \phi') ( \phi-\beta \phi')^\top \\
& = \norm{\phi}^2  \left\{ \phi \phi^\top-\beta (\phi' \phi^\top + \phi (\phi')^\top) + \beta^2 \phi' (\phi')^\top \right\}\,,
\end{align*}
which does not match the previous display.
The terms that do not match are the linear-in-$\beta$ terms.
In the first display we have $ -2 \beta \phi' (\phi')^\top$, while in the bottom we have $-\beta (\phi' \phi^\top + \phi (\phi')^\top)$.

The first equality of their display states (in equivalent form) that 
\begin{align*}
A^\top A = \norm{\phi}^2 ( \phi \phi^\top - 2\beta \phi (\phi')^\top +\beta^2 \phi' (\phi')^\top )\,.
\end{align*}
We see that while this is closer to the correct result, here the mistake is that $-2\beta \phi  (\phi')^\top$ is replacing $-\beta(\phi (\phi')^\top + \phi' \phi^\top)$.

We also have a hard time following the rest of the proof of this lemma.
In fact, we have a simple proof for a result similar to stated in this lemma.
This works by observing that one needs to 
bound the Lipschitz factor of $\theta \mapsto \bar T_n \dots \bar T_{t+1} ( (I-\gamma_t A_t)\theta + b_t )$, where
$A_t = \phi_t (\phi_t -\beta \phi_t')^\top$, $b_t = r_{i_t} \phi_t$ and
 $\bar T_t(\theta) = (I-\gamma_t \bar A_T) \theta + \bar b_T$ (we assume that the stepsize sequence is deterministic).
 Clearly, this Lipschitz factor can be bounded by $\prod_{s=t+1}^n \norm{I-\gamma_s \bar A_T}_2 \norm{I-\gamma_t A_t}_2$
 (the composition of Lipschitz maps is Lipschitz with a factor that is the product of the individual maps' Lipschitz factors). 
Invoking (A3) then indeed gives some version of Lemma 7.
By making an appropriate assumption (similar to those used in our paper), 
$\prod_{s=t+1}^n \norm{I-\gamma_s \bar A_T}_2 $ can be controlled.

 
\section{Bounded Linear Space Assumption}
\citet{flstda} assume that the parameter vector stays such that the value function $\Phi \theta$ (with their notation) will be bounded in $L^\infty$-norm (see assumption \textbf{$(A3)$} of \cite{flstd} and \cite{flstda}). This assumption is critical in establishing Lemma~7 (see pages 15 and 16, \cite{flstda}).
We suspect the following shortcomings with assumption \textbf{$(A3)$}
\begin{itemize}
\item The assumption is stated in a somewhat sloppy fashion. We take the authors meant to say that $\sup_n \norm{\Phi \theta_n}_\infty<+\infty$ holds almost surely. This seems like a strong assumption! Would not one of the main question be whether this assumption even holds? We see in our paper that for some cases, the iterates can blow up if the step-size is inappropriately selected!
\item One possibility would be to modify the algorithm by adding a projection step to guarantee boundedness.
It is still unclear to us whether this alone would ensure convergence of the error to zero. In any case, the analysis is invalidated if a projection step is present (basically, the algebraic identities will all fail to hold).
%For the sake of an argument let us also additionally assume that the iterates are bounded by a projection step (which  \citet{flstda} don't mention explicitly). Then would the recursion unfold the same way as it does in Equation (26) of \cite{flstda} (or Equation (13) of \cite{flstd})? How are we to ensure decay/contraction (as shown in Equation (27) of \cite{flstda}) at each step (since projection might be non-contracting)?
%\item The recursion eventually (see Equation (26) of \cite{flstda}) has only \emph{additive noise} and not \emph{multiplicative noise} as analyzed in our submission. The additive noise is captured in the form of the $H_\beta^2$ term in Equation (28) of \cite{flstd}. This reduction to additive noise is possible due to \textbf{$(A3)$}.
\end{itemize}
\section{Relation between Covariance Matrix and $\bar{A}_T$ matrix}
\citet{flstda} assume positive definiteness of \textbf{$(A4)$} covariance matrix $\frac{1}{T}\Phi^\top_T \Phi_T$. However, unlike regression problems, in reinforcement learning problems what appears in the recursion (see Equation (6)) is not the covariance matrix, but a different matrix $\bar{A}_T=\frac{1}{T}\sum_{i=1}^T \phi(s_i)(\phi(s_i)-\beta\phi(s_i'))^\top$ defined in pages 2, 4 (below Equation (5)), 8 and 16 of \cite{flstda}. Usually, without a sampling assumption known as the `on-policy' case (see \cite{gtd} for a discussion on `on-policy' vs `off-policy') the eigenvalues of $\bar{A}_T$ cannot be guaranteed to have all positive real parts. While \citet{flstda} mention the `on-policy' sampling in the introduction, there is no explicit sampling assumption in the list of assumption. In fact, we doubt that the proposed algorithm will converge without extra assumption (as discussed above).
\if0
This is worrisome for the following reasons
\begin{itemize}
\item \textbf{$(A4)$} deals only with the covariance matrix and it is not clear how that alone is sufficient to say that $\bar{A}_n-(1-\beta)\mu$ is positive definite (see below Equation (26) page 16 of \cite{flstda}; the problem mentioned earlier).
\item For the sake of the argument, let us suppose that $(1-\beta)\mu$ is the smallest eigenvalue of $\bar{A}_n$. However, the problem here is that $\bar{A}_n$ is not symmetric in general and it is typical in reinforcement learning applications to say a non-symmetric matrix $M$ is positive definite if $x^\top M x>0$ for all real valued vectors $x$. While it follows from this notion of positive definiteness of $M$ that all the eigenvalues of $M$ have positive real parts, there could still be eigenvalues with complex conjugate parts. Hence to assume that the smallest eigenvalue of $\bar{A}_n$ to be $(1-\beta)\mu$ might be problematic (because there could be other complex eigenvalues with smaller magnitude). While it might be true in practice that the smallest eigenvalue of $\bar{A}_n$, is greater than $(1-\beta)\mu$ to our knowledge there is no proof for this fact in literature.
\end{itemize}
\fi
\section{Blow Up of the Bound}
We would like to note that the rate expression in Corollary~4 of \cite{flstda} (or Corollary~2 of \cite{flstd}) contains a constant $C$. The authors do mention that the sampling error blows up as $\alpha \rightarrow 1$.
However, it also looks like that even the constant $C=\sum_{n=1}^\infty \exp(-\mu cn^{1-\alpha})$ will blow up as $\alpha\rightarrow 1$, in which case it is not clear whether a $1/\sqrt{n}$ rate is achieved (even if we disregard all the other problems).

\section{Doubt about the Effectiveness of the Algorithm}
In Corollary~4 of \cite{flstda} (or Corollary~2 of \cite{flstd}), we learn that the value of $c$ governing the stepsize of the primary update must be in a small range (it must be between $1.33$ and $2$). This means, that effectively, the stepsize $\gamma_n$ behaves as $1/n^\alpha$ ($c$ has very little effect). At least when $\alpha=1$, we know that stepsizes like this make the bias decrease slowly and averaging remains ineffective. This seems to be at odds with the suggestion after this result that $\alpha\to 1$ is a desirable choice. In fact, we would be inclined to choose $\alpha=1/2$, i.e., its lowest value. This is because then the bias is decreased relatively quickly, while the variance will be controlled by the additional averaging. However, given all the problems with this it remains to be seen whether this is indeed a reasonable choice and under exactly what conditions this is reasonable.
\bibliographystyle{plainnat}
\bibliography{issuesref}
%\newpage
%\input{linal}




%$f_m(\theta_{m-1})=(\theta^\top \phi(s_{i_m})-(r_{i_m}+\beta\theta^\top \phi(s_{i_{m+1}})))$
\end{document}

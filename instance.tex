%!TEX root =  flsa.tex
\section{Main Results and Discussion}\label{sec:mainresults}
In this section, we derive instance dependent bounds that are valid for a given problem $P$ (satisfying \Cref{assmp:lsa}) and in the \cref{sec:uniform}, we address the question of deriving uniform bounds $\forall\,P\in \P$, where $\P$ is a class of distributions (problems). Here, we only present the main results followed by a discussion. The detailed proofs can be found in \cref{sec:proofs}. 
\todoc{Usually it is expected that one gives an outline of the proof technique. Especially in a theory paper. At least verbally we could explain the tools used, how the proof is similar and/or different to previous proofs.}
In what follows, for the sake of brevity, we drop the subscript $P$ in the quantities $\EEP{\cdot}$, $\sigma^2_{A_P}$ and $\sigma^2_{b_P}$. \todoc{I dropped $P$ from $\EEP{\cdot}$ before. Either put there back, or remove $\EEP{\cdot}$ from here.}
%\begin{lemma}\label{lm:simtran}[Similarity Transformation]
%Let $P$ be Hurwitz, then there exists a $U\in \gln$ such that $P_U$ is positive definite.
%\end{lemma}
We start with a lemma, which is needed to meaningfully state our main result:
\begin{lemma}\label{lm:hur}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}. 
Then there exists an $\alpha_{P_U}>0$ and $U\in \gln$ such that $\rhod{P_U}>0$ and $\rhos{P_U}>0$
holds for all $\alpha \in (0,\alpha_{P_U})$. 
\end{lemma}
\begin{comment}
\begin{proof}
\begin{align*}
\rhos{P}&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^*\EE{A_t^* A_t} x\\
&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^* A^*_P A_P -\alpha x^* \EE{M_t^* M_t} x\\
&\geq \lambda_{\min}(A^*_P+A_P)-\alpha \norm{A_P}^2-\sigma^2_P
\end{align*}
The proof is complete by choosing $\alpha_P<\frac{\lambda_{\min}(A^*_P+A_P)}{\norm{A_P}^2+\sigma^2_P}$
\end{proof}
\end{comment}


\begin{comment}
\begin{theorem}\label{th:pdrate}
Let $P^A$ in \Cref{assmp:lsa}-~\ref{dist} be a positive definite distribution over $\C^{\dcd}$. Then
\begin{align*}
\EE{\norm{\thh_t-\ts}^2}
\leq
\left(1+\frac2{\alpha\rhod{P}}\right)\frac{1}{\alpha \rhos{P}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\theta_0-\ts})}{t+1} \right)\,.
\end{align*}
\end{theorem}

\begin{theorem}\label{thm:simtran}[Change of Basis]
Let $P$ in \Cref{assmp:lsa}-~\ref{dist} be AS. Define $\gamma_t\eqdef U^{-1}\theta_t,\,,\gamma^*\eqdef U^{-1}\ts$, and suppose if $P_U$ is positive definite, then
\begin{align*}
\EE{\norm{\gh_t-\gamma^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U^{-1}}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\gamma_0-\gamma^*}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\gamma_0-\gamma^*})}{t+1} \right)\,,
\end{align*}
where $\gh_t=\frac{1}{t+1}\sum_{s=0}^t \gamma_s$.
\end{theorem}
\end{comment}

\begin{theorem}\label{th:rate}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}.
Then, for  $U\in\gln$ and $\alpha_{P_U}>0$ as in \cref{lm:hur},
\todoc{Why not use $\alpha_P$? $U$ is dependent on $P$. If $\alpha_{P_U}$ does not have a specific definition, there is no point to denote this dependence on $U$. It just adds to the clutter.}
for all $\alpha\in (0,\alpha_{P_U})$ and for all $t\ge 0$,
\begin{align*}
\EE{\normsm{\thh_t-\ts}^2}
\leq
\nu_P\,
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,,
\end{align*}
where $\nu_P = \left(1+\tfrac2{\alpha\rhod{P_U}}\right)\tfrac{\cond{U}^2}{\alpha \rhos{P_U}}$.
\end{theorem}


\begin{theorem}[Lower Bound]\label{th:lb}
There exists a distribution $P$ over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}, such that
there exists $\alpha_P>0$ so that $\rhos{P}>0$ and $\rhod{P}>0$ hold for all $\alpha\in (0,\alpha_P)$ and
\begin{align*}
\EE{\norm{\thh_t-\ts}^2} &\geq \frac{1}{(t+1)^2}(\alpha)^{-2}(\rhod{P}\rhos{P})^{-1}\Big\{ \big(1-(1-\alpha \rhos{P})^t\big) \norm{\theta_0-\ts}^2 \\&+ \sum_{s=1}^t \big(1-(1-\alpha \rhos{P})^{(t-s)}\big) \big(\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}\big) \Big\}
\end{align*}

\begin{comment}
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}
\end{comment}
\end{theorem}

\begin{comment}

Consider the LSA with $b_t=0$, and $P^0$ has all the mass concentrated on $A_{P^0}=\begin{bmatrix} \lambda_{\min} &0\\ 0& \lambda_{\max}\end{bmatrix}$, for some $\lambda_{\max}>\lambda_{\min}>0$. Note that in this example $\ts=0$.
By choosing $\alpha<\frac2{\lambda_{\max}}$, in this case it is straightforward to write the expression for $\eh_t$ explicitly as below:
\begin{align*}
\eh_t&=\frac{1}{t+1}\sum_{s=0}^t e_t = \frac{1}{t+1}\sum_{s=0}^t (I-\alpha A_P)^{t-s} e_0\\
&=\frac{1}{t+1}(\alpha A)^{-1}\left(I-(I-\alpha A)^{t+1}\right)e_0
\end{align*}

\begin{align*}
\norm{\eh_t}^2&=\frac{1}{(t+1)^2}\norm{(\alpha A_P)^{-1}\left(I-(I-\alpha A_P)^{t+1}\right)e_0}^2
&\leq\frac{1}{(t+1)^2}\alpha^2\norm{A_P^{-1}}^2\norm{e_0}^2
\end{align*}

\begin{align*}
\norm{\eh_t}^2\geq \eh^2_t(1)=\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)
\end{align*}
Note that in this example, $\rhos{P^0}=\rhod{P^0}=\lambda_{\min} -\alpha \lambda_{\min}^2=\lambda_{\min}(1-\alpha \lambda_{\min})$. Sandwiching the error between the lower and upper bounds we have
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}
\end{comment}
In what follows, we discuss the specific details of these results. 
\vspace*{0.5em}
%\subsection{Result Discussion}
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item \textbf{Role of $U$:} When the given problem $P$ is positive definite, it is straightforward to produce an $\alpha_P>0$ such that $\rhos{P}>0$ and $\rhod{P}>0,\,\forall \alpha\in(0,\alpha_P)$ by considering the definitions of $\rhos{P}$ and $\rhod{P}$. 
However, the problem $P$ being Hurwitz does not guarantee positive definiteness (see \Cref{ex:pdas}). 
Following  \cite{lihong}, we remedy this 
by applying a similarity transformation using an appropriate $U$ that transforms a Hurwitz problem $P$ to a positive definite problem $P_U$. To see how this works, let $\gamma_t\eqdef U^{-1 }\theta_t$. Premultiplying 
 \eqref{conststep} by $U^{-1}$ and plugging in $\gamma_t$, we get
\begin{align*}
%U^{-1}\theta_t&=U^{-1}\theta_{t-1}+\alpha(U^{-1} b_t- U^{-1}A_t U U^{-1}\theta_{t-1}),\\
\gamma_t&=\gamma_{t-1}+\alpha(U^{-1} b_t- J_t\gamma_{t-1}),
\end{align*}
where $J_t = U^{-1}A_t U$.
Now, since any Hurwitz matrix is similar to a PD matrix with an appropriate similarity transformation $U$, choosing $U$ this way we get that $\E[J_t]$ is PD, hence $P_U$ is PD.
%This completes the transformation of a Hurwitz problem $P$ to a positive definite problem $P_U$. 
The condition number $\cond{U}$ appears in our bound because $\theta_t$ is recovered from $\gamma_t$ using
$\theta_t=U\gamma_t$. Of course, when the problem $P$ is positive definite, we can let $U=\I$ in \Cref{th:rate}. 
\item \textbf{Role of $\rhos{P}$ and $\rhod{P}$:} When $P$ is positive definite, we can expand the MSE as \begin{align}\label{eq:exp}\EE{\norm{\eh_t}}=\frac{1}{(t+1)^2}\ip{\sum_{s=0}^t e_s,\sum_{s=0}^t e_s}.\end{align} The inner product in \eqref{eq:exp} is a summation of \emph{diagonal} terms $\EE{\ip{e_i,e_i}}$ and \emph{cross} terms of$\EE{\ip{e_i,e_j}},i\neq j$. The growth of the diagonal terms and the cross terms depends on the spectral norm of the random matrix $H_t=I-\alpha A_t$ and the deterministic matrix $H_P=I-\alpha A_P$ respectively. These are given by
\begin{comment}
The output $\thh_t$ of the algorithm \eqref{eq:lsa} is the average of the internal states at times $s=0,\ldots,t-1$. The error dynamics of the internal states by looking at the behavior of $e_t\eqdef \theta_t-\ts$.
\begin{align}\label{eq:errec}
\theta_t&=\theta_{t-1}+\alpha(b_t-A_t\theta_{t-1})\nn\\
\theta_t-\ts&=\theta_{t-1}-\ts+\alpha\left(b_t-A_t(\theta_{t-1}-\ts+\ts)\right)\nn\\
e_t&=e_{t-1}+\alpha(b_t-A_t e_t -A_t\ts)\nn\\
e_t&=\underbrace{((I-\alpha A_t))}_{\text{Random-Matrix}} e_{t-1}+\underbrace{\alpha(N_t -(M_t)\ts)}_{\text{Noise}}
\end{align}
From \eqref{errec} it is clear that the error dynamics depends on the $i)$ properties of the random matrix, $ii)$ properties of the noise. In the absence of the noise term in \eqref{eq:errec}, we have $e_t=(I-\alpha A_t) e_{t-1}=\Pi_{s=1}(I-\alpha A_s) e_0$, i.e., the initial error is get multiplied by a product of random matrices. In such a scenario, we can guess that whether or not the algorithm forgets the bias $\norm{\theta_0-\ts}$, depends on whether the matrix product is contracting. One way to characterize this contracting property is to look at the spectral radius of the random matrix, which is given by
\end{comment}
\begin{align}\label{eq:spectralrand}
\EE{\norm{I-\alpha A_t}^2}=1-\alpha \rhos{P}, {\norm{I-\alpha A_P}^2}=1-\alpha \rhod{P}
\end{align}
Note that \eqref{eq:spectralrand} explicitly connects $\rhod{P}$ and $\rhos{P}$ in \Cref{def:dist} to the spectral norms matrices $H_t$ and $H_P$. For the MSE to be bounded, we need the spectral norms to be less than unity, which in turn translates to the requirement that $\rhos{P}>0$ and $\rhod{P}>0$. If $P$ is Hurwitz, we can argue on similar lines by first transforming it into a positive definite problem $P_U$ and replacing $\rhos{P}$ and $\rhod{P}$ by $\rhos{P_U}$ and $\rhod{P_U}$, and introducing $\cond{U}$ to account for the forward ($\gamma=U^{-1}\theta$) and reverse ($\theta=U\gamma$) transformations using $U^{-1}$ and $U$ respectively.
\item \textbf{Constants} {$\alpha$, $\rhos{P}$ and $\rhod{P}$} do not affect the exponents $\frac{1}{t}$ for variance and $\frac{1}{t^2}$ for bias terms.
\item \textbf{Bias and Variance:} The means-squared error at $t$ is bounded by a sum of two terms. The first term is the bias term given by $\B=\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\cond{U}^2}{\alpha \rhos{P_U}}\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}$.  The \emph{bias} term that captures the rate at which the initial condition $\norm{\theta_0-\ts}^2$ is forgotten. The second term is the \emph{variance} given by $\V=\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\cond{U}^2}{\alpha \rhos{P_U}} \frac{\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} $. The variance term that captures the rate at which noise is rejected.
\item \textbf{Behaviour for extreme value of $\alpha$:} For smaller values of step-size, i.e., $\alpha\approx 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ term. This is due to the fact that the stepsizes determine the learning rate and for smaller stepsizes the learning rate is slower. However, in this case the noise term does not blow up, a fact that can appreciated by looking at \Cref{th:rate} where $\alpha$ is seen to multiply the variances $\sigma^2_A$ and $\sigma^2_b$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms can each other. For larger values of $\alpha$ i.e., $\alpha\ra \alpha_{P}$ (or $\alpha_{P_U}$ as the case may be), the bounds blow up again, due to the fact that $\rhos{P}\ra 0$ in this case.
\item \textbf{Lower Bound} in \Cref{th:lb} shows that the factor $\rhos{P}\rhod{P}$ appearing in the upper bound in \Cref{th:rate} is inevitable for general LSA with CS-PR. However, there are specific examples such as SGD for the linear regression problem considered by \citet{bach} where this factor can be avoided (for further remarks see \Cref{sec:related}).
\end{itemize}
\input{illustrate}

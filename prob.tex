\section{Problem Setup}\label{sec:prob}
We consider linear stochastic approximation algorithm (LSAs) with constant step-size (CS) and Polyak-Ruppert (PR) averaging of the iterates given as below:
\begin{subequations}\label{eq:lsa}
\begin{align}
\label{conststep}&\text{LSA:} &\theta_t&=\theta_{t-1}+\alpha(b_t-A_t\theta_{t-1}),\\
\label{iteravg}&\text{PR-Average:} &\thh_t&=\frac{1}{t+1}{\sum}_{i=0}^{t}\theta_i,
\end{align}
\end{subequations}
where $\thh_t, \theta_t \in \R^{d}$. The algorithm updates a pair of parameters $\theta_t,\tb_t\in \R^{\dcd}$ incrementally, in discrete time steps $t=1,2,\dots$
based on data $b_t\in \R^d$, $A_t\in \R^{\dcd}$. Here $\alpha>0$ is a positive step-size parameter; the only tuning parameter of the algorithm besides the
initial value $\theta_0$. The iterate $\theta_t$ is treated as an internal state of the algorithm, while $\thh_t$ is the output at time step $t$. The update of $\theta_t$ alone is considered a form of constant step-size LSA.\par
Note that the computation of $\theta_t$ can also be performed in an incremental fashion using $O(d)$ storage. Owning to the special form of $A_t$, sometimes the matrix-vector product $A_t \theta_{t-1}$ can be efficiently computed in $O(d)$ time. This happens for example when $A_t$ is rank one; examples of this kind arising in reinforcemnet learning \cite{sutton,konda-tsitsiklis,gtd,gtd2,gtdmp} will be presented in \Cref{sec:related}. The significance of efficient computation of the matrix-vector product is that an update of the algorithm
can then be implemented in $O(d)$ time and $O(d)$ storage, which makes the algorithm particularly attractive in large-scale computations when $d$ is in the range of thousands or more.\par
We are interested in the behaviour of \eqref{eq:lsa} under the assumptions presented in \Cref{assmp:lsa}. In what follows, we make use of the $\sigma$-fields $\F_{t-1}\eqdef\{\theta_0,A_1,\ldots, A_{t-1}, b_1,\ldots, b_{t-1}\}$, and $\F_0$ that holds all random variables.
\begin{assumption}\label{assmp:lsa}
\begin{enumerate}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item \label{dist} $(b_t, A_t)\sim (P^b,P^A), t\geq 0$ is an $i.i.d$ sequence, where $P^b$ is a distribution over $\R^d$ and $P^A$ is a distribution over $\R^{\dcd}$. We assume that $P$ is Hurwitz.
\item \label{matvar} The martingale difference sequences\footnote{$\EE{M_t|\F_{t-1}}=0$ and $\EE{N_t|\F_{t-1}}=0$} $M_t\eqdef A_t-A_{P}$ and $N_t\eqdef b_t-b_{P}$ associated with $A_t$ and $b_t$ satisfy the following
\begin{align*}\E\left[ \norm{M_t}^2\mid\F_{t-1}\right]\leq \sigma^2_{A_P}, \, \E[N_t^* N_t]=\sigma^2_{b_P}.\end{align*}
\item $A_P$ is invertible and there exists a $\ts=A^{-1}_Pb_P$.
\end{enumerate}
\end{assumption}
%For the rest of the paper we denote the errors of the internal and the output variables of \eqref{eq:lsa} at time $t$ by $e_t\eqdef \theta_t-\ts$ and $\eh_t\eqdef \thh_t-\ts$ respectively.
\paragraph{Performance Metric:}  We are interested in the behavior of the mean squared error (MSE) at time $t$ given\footnote{ $\EEP{\norm{\thh_t-\ts}_C^2}$, the MSE with respect a hermitian matrix $C$ is desirable, we cannot exploit the use of $C$ unless it is connected to $P$ in a special way. For more discussion, see \Cref{sec:related}.}
 by $\EE{\norm{\thh_t-\ts}^2}$.
\begin{comment}
\footnote{We note here that $\EE{P\norm{\thh_t-\ts}_C^2}$ which measures the MSE in the quadratic norm with respect to a positive definite matrix $C\succ 0$. However, unless $C$ has some special structure, it is not possible to exploit this generality in our bounds. Further, when data is bounded $\norm{C}^2$ is also bounded and it is straightforward to see that $\EEP{\norm{\thh_t-\ts}}^2_C\leq \norm{C}^2\EEP{\norm{\thh_t-\ts}^2}$. So, for the purpose of stating our results we will not use the general quadratic norm, however, will use it when we discuss the work by \cite{bach} in the linear regression setting, where its usage is relevant.}
\end{comment}
%We wish to understand how the MSE is affected by the choice of $\alpha$ and the spectral properties of $A_P$. Another aspect of interest is to understand the contribution of the \emph{bias} (caused by the initial condition $\theta_0$) and the \emph{variance} (caused by $\sigma^2_P,\sigma^2_b$) to the MSE.\par
\todoch{Mention importance of forgetting the bias etc. Mention that we really don't have control over $\norm{U}$}

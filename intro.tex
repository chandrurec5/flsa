\section{Introduction}
\begin{comment}
Stochastic Approximation (SA) algorithms can be expressed in the general form
\begin{align}\label{eq:sa}
\theta_t=\theta_{t-1}+\alpha_t f(\theta_{t-1},\zeta_t),
\end{align}
where $\theta_t\in \R^d$ are the iterates, and $f(\theta_{t-1},\zeta_t)$ of the function $g(\theta_{t-1})=\EE{f(\theta_{t-1})}_{\zeta_t}$, where the expectation is with respect to some noise process $\zeta_t$. At each time step, the algorithm updates the iterates in increments proportional to a step-size $\alpha_t>0$ and the idea is to eventually converge to $\ts\in \R^d$ which is a desired optimum or a fixed point. Due to their inherent online nature SA algorithms appear widely in machine learning applications \cite{konda,bhatnagar,borkar,gtd,td,bach}, where we have to learn from a noisy data stream.\par
The choice of the step-size $\alpha_t$ needs to trade-off learning rate versus filtering noise, so to speak. In general, one can guess that larger step-size lead to faster learning and at the same time smaller step-sizes are better to handle noise. A popular choice of step-size schedule that is known handle the noise $\alpha_t=O(\frac{1}{t})$. However, since the step size are diminishing, it can hurt the learning rate. An alternative suggestion is to use more aggressive step sizes without minding the noise in the iterates $\theta_t$ in \eqref{eq:sa}, however, introduce additional averaging (known as Polyak-Rupert averaging) to filter the noise. The averaged iterates can be given by $\th_t\eqdef\frac{1}{t}\sum_{s=0}^t \theta_t$, and an analysis of the PR-averaging technique is known from \cite{}.\par
\end{comment}

A linear stochastic approximation (LSA) algorithm of the form given by
\begin{align}\label{eq:lsaintro}
\theta_t=\theta_{t-1}+\alpha_t (b_t-A_t \theta_{t-1}),
\end{align}
where the iterates $\theta_t\in \R^d$ and step-sizes $\alpha_t>0$ are quantities related to the algorithm, while $b_t\in \R^d$ and $A_t\in \R^{\dcd}$ is the data sequence presented to the algorithm.
LSA algorithms are useful in machine learning and reinforcement learning (RL) applications, where the aim is to compute a $\ts\in \R^d$ using the noisy data sequence $(b_t,A_t)$ using cheap per iteration computations. Specific examples include the stochastic gradient descent algorithm for the problem of linear predition with quadratic loss, and the temporal difference class (TD) of learning algorithms in RL.\par
%An additional feature in these class of pplications is that $A_t$ turns out to be a rank-$1$ matrix and $A_t\theta_{t-1}$ can be obtained in $O(d)$, which is attractive due to the cheap per time step computational requirement.\par
It is of interest to understand the effect of the step-sizes (a quantity that can be controlled by the algorithm designer) on the performance of the LSA. The choice of the step-sizes $\alpha_t$ needs to trade-off learning rate versus filtering noise, so to speak. In general, one can guess that larger step-sizes might lead to faster learning and at the same time smaller step-sizes are better at handling the noise. A technique in literature that goes back to \cite{polyak-judisky,ruppert} is to use a constant step-size (CS) $\alpha_t=\alpha>0,\forall t\geq 0$  in conjunction with averaging of iterates , known as Polyak-Rupert (PR) averaging to filter the noise. The averaged iterates can be given by $\th_t\eqdef\frac{1}{t}\sum_{s=0}^t \theta_t$. Under the CS-PR technique $\theta_t$ is the internal vairable and $\thh_t$ is the output of the algorithm. Intuitively, the CS-PR technique can be seen to address the learning vs filtering trade-off, where the constant step-size takes care of the learning  and the averaging of the iterates takes care of the filtering. \par
Recently \cite{bach} considered stochastic gradient descent algorithm (SGD)\footnote{SGD is a special of the LSA algorithm in \eqref{eq:lsaintro}.} with CS-PR for the linear prediction problem with squared loss and $i.i.d$ sampling. A ``remarkable'' result by \cite{bach} is that there exists a constant step-size such that for any problem where the noisy data is bounded by a known constant, the expected squared prediction error after $t$ updates is at most $\frac{C}{t}$ with a constant $C>0$ that depends \emph{only} on the bound on the data.\todoc{Dimension?}
\paragraph{Motivation and Challenges:} In this paper, we wish to study the effect of CS-PR in the case of general LSA algorithms of form \eqref{eq:lsaintro}. Our study is motivated by the problem of linear value function approximation from experience replay in a batch setting in RL using TD class of algorithms. An important difference between the linear prediction problem considered by \cite{bach} and the general LSA setting we consider is that in the former has certain special strucutres and the latter does not. One of the special strucutre exploited by \cite{bach} is the fact that $A_t$ are real symmetric matrices. The general LSA setting does not enjoy such an additional structure presents an interesting challenge.\par
\paragraph{Contributions:} We now list our contributions in this paper
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item We show that in the case of general LSA, there exists instance dependent constant step size choice such that the mean squared error (MSE) of the CS-PR procedure given by $\EE{\norm{\thh_t-\ts}^2}$ is at most $O(\frac{1}{t})$. The MSE further can be broken down to a \emph{bias} term (arising due to the initial condition $\theta_0$) that decays at a rate $O(\frac{1}{t^2})$ and a \emph{variance} term (due to the noise) that decays at at rate $O(\frac{1}{t})$.
 \item We also show (via counter example) that not all classes of general LSAs `admit' a constant step-size that is uniform across all the problems in the class. Further, we also argue that the subclass of LSAs related to linear prediction problem does `admit' a problem independent constant step size.
\item We observe that when the constant step-size is chosen too big it results in instability, however, a conservative choice of the constant step-size affects only the constants in our error bounds without affecting the asymptotic rate (which might not be true with other popular choices of step-sizes). We aruge heuristically that CS-PR procedure can be deployed in a pricinpled manner by using an outer loop that automatically reduces that constant step-size by detecting detects the blow up of iterates.
\end{itemize}
\paragraph{Highlights:}
 We only assume that $\EE{A_t}$ is Hurwitz, i.e., all its eigenvalues have positive real parts. This assumption is quite common in literatures and appreas in the works by \cite{polyak-judisky,borkarbook}. Eventhough our proof technique resembles that by \cite{bach}, in addition, we make use of Jordon/Cyclic decomposition in a critical way to handle the general Hurwitz assumption. Our results are applicable to wide variety of TD algorithms such as TD(0), GTD/GDT2, TDC. Further, the error dynamics has a simple interpretation in terms of behaviour of the sum of terms in a geometric series, and hence offers a simple and unified understanding of the various TD algroithms.
\paragraph{Organization:}
\begin{comment}
A host of algorithms in machine learning and in particular, the temporal difference class of algorithms arising in reinforcement learning, turn out to be special cases of \eqref{eq:sa}, namley linear stochastic approximation (LSA) algorithms. LSA algorithms assume the general form
 Further, in most instances of LSA $A_t$ turns out to be a rank-$1$ matrix and $A_t\theta_{t-1}$ can be computed in $O(n)$, and are desired for this low per iteration computational cost. Recently, \cite{bach} used \eqref{eq:lsaintro} with constant step-size, i.e., $\alpha_t=\alpha,\,\forall t\geq 0, \, \alpha >0$ and PR-averaging for the linear prediction problem with squared loss and $i.i.d$ sampling. A ``remarkable'' result by \cite{bach} is that there exists a constant step-size such that for any problem where the noisy data is bounded by a known constant, he expected squared prediction error after $t$ updates is at most $\frac{C}{t}$ with a constant $C>0$ that depends \emph{only} on the bound on the data.\todoc{Dimension?}

In particular, as opposed to many earlier results available in the literature \todoc{Add citations}, $C$ does \emph{not} depend on the conditioning of the underlying system, while the total runtime of the algorithm scales linearly with both $t$ and $n$.\par
In this paper, we want to understand the performance of constant step size with PR-average of iterates for general class of LSAs (outside of the linear prediction setting considered by \cite{bach}). We are interested in the problem of linear value function approximation in reinforcement learning using temporal difference learning (TD) either from experience replay in a batch setting or solving linear systems using TD-style algorithms.
\todoc{TD-style, or just TD?} \todoc{Later we need to comment on TD with eligibility traces and other variations of TD, e.g., new variants.} We now list our contributions, and its implications to the TD class of algorithms, and the key differences between the techniques employed here in comparison to \cite{bach}. We now list our contributions, the implications of our results, and the challenges presented by the general setting in comparison to the linear prediction setting considered by \cite{bach}.
\paragraph{Contributions}:
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item We show that in the case of general LSA, there exists instance dependent constant step size choice such that the mean squared error of the PR-average given by $\err_t=\EE{\norm{\thh_t-\ts}^2}$ is at most $O(\frac{1}{t})$. The error $\err_t$ further can be broken down to a \emph{bias} term (arising due to the initial condition $\theta_0$) that decays at a rate $O(\frac{1}{t^2})$ and a \emph{variance} term (due to the noise) that decays at at rate $O(\frac{1}{t})$.
\item We also show (via counter example) that not all classes of general LSAs `admit' a constant step-size that is uniform across all the problems in the class. Further, we also argue that the subclass of LSAs related to linear prediction problem does `admit' a problem independent constant step size.
\item The assumptions in our setting is comparable to those in the asymptotic analysis based on ordinary differential equation (ODE) method by \cite{borkarmeyn,borkarbook}.
\item Two timescale LSA algorithms are those that use two set of iterates (namely slower and faster) and two separate step-size schedules. Our results also hold for two timescale LSAs as long as the two different step-sizes are held constant with respect to time. Thus our setting not only holds for TD(0) but also extends to GTD/GTD2 and TDC.
\item On an intuitive level, the error dynamics of the constant step-size plus PR-averaging procedure is similar to the behavior of partial sums of a geometric series. This coupled with the fact that the result also encompasses two timescale LSAs presents a simple and unified view of the various TD algorithms.
\item The break up of error into bias and variance terms and their respective rates were previously worked out by \cite{bach} in the linear prediction setting, and here we extend the same to the general class of LSAs. Nevertheless, there are certain key differences between the general LSA setting that we consider to the setting considered by \cite{bach}, which are listed as under.
\end{itemize}
\end{comment}
\begin{comment}
\paragraph{Implications:}
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item Our results hold under the weak assumption\footnote{When this weak assumption fails to hold, the LSA is unstable and hence the notion of convergence is void.} that all the eigen values of $\EE{A_t}$ have strictly real parts. This assumption holds for all known TD class of RL algorithms.
\item Two timescale LSA algorithms are those that use two set of iterates (namely slower and faster) and two separate step-size schedules. Our results also hold for two timescale LSAs as long as the two different step-sizes are held constant with respect to time. Thus our setting not only holds for TD(0) but also extends to GTD/GTD2 and TDC. This provides a unified view of the various temporal difference learning algorithms.
\item The intuition behind the error analysis can be explained in simple terms when the noise free case is considered (i.e., $A_t=A$ in \eqref{eq:lsaintro}). In this case, the LSA is just a linear time invariant dynamical system where the error $\theta_t-\ts=(I-\alpha A)^t (\theta_0-\ts)$ and $\norm{\theta_t-\ts}=\rho^t\norm{\theta_0-\ts}$ (for some $0<\rho<1$). Now the PR-average is given by $\frac{\norm{\theta_t-\ts}^2}{t^2}=\frac{\sum_{s=0}^{t-1}\rho^s}{t^2} \norm{\theta_0-\ts}^2$. Note that the numerator is just sum of terms of a geomteric series, which is summable as long as the matrix $(I-\alpha A)$ has eigen values within the unit circle.
\item The fact that the setting covers even two timescale LSAs with constant step size, coupled with the intuitive interpretation of the error dynamics together present a simple and unified view of the various temporal difference learning algroithms in literature.
\item One of the major bottlenecks in selecting step-size for general LSA is the lack of problem knoweldge. One way to handle this issue is to choose a diminishing step size schedule such as $\alpha_t=\frac{1}{t}$. However, such a strategy can fail when the underlying matrices are ill-conditioned (see \Cref{sec:stepsizes}). Another popular approach has been to choose $\alpha_t=\frac{c}{c+t}$, where initially the step-sizes are held almost constant and then as $t$ increases they decay at rate $O(\frac{1}{t})$. While this approach can mitigate the ill-conditioning problem, it can at the same time amplify the effect of noise. In this paper, we argue informally that the constant step-size with PR-averaging of iterates can be a promising way to mitigate these issues, by periodically checking for possible blow up of the norm of the iterates and then reducing the step-size based on this information (see \Cref{sec:stepsizes}).
\end{itemize}
\paragraph{Challenges:} The break up of error into bias and variance terms and their respective rates were previously worked out by \cite{bach} in the linear prediction setting, and here we extend the same to the general class of LSAs. Nevertheless, there are certain key differences between the general LSA setting that we consider to the setting considered by \cite{bach}, which are listed as under.
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item In the linear least-squares problem is considered by \citet{bachharder} and the random matrices involved are known to be symmetric positive definite. This enables \citet{bachharder} to define operators that act between spaces of \emph{symmetric} matrices and carry out the computations making use of such operators. In our case, the matrices are known only to be (laxly) positive definite, i.e., they lack symmetry. \todoc{We should point out after our analysis in which step the symmetry helps \citet{bachharder}. We should also make this somehow clear here if possible.}
Thus, the above approach does not work, and instead we have resort to an analysis that makes use of only the expected norms of the random matrices involved.
\item Another significant difference is that in linear least-squares, the ``noise'' has a favorable ``structure''. Again, due to the lack of symmetry, this favorable structure does not apply. \todoc{Again, after the analysis, we should point out how the structured noise would have helped. I am wondering whether a more graceful degradation should happen? Is this really all or nothing?}
\end{itemize}
\end{comment}

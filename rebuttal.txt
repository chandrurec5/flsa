We thank the reviewers for their comments.
We would like to start with answering what appears to be the main issue raised by Reviewer #3.
1) Missing citation to Prashanth, Korda and Munos' ECML'14 paper.
In fact, Prashanth and Korda later published arxiv.org/pdf/1411.3224.pdf (ICML'15), which we thought was the "state-of-the-art" and we cited this paper. Now, strangely this paper does not cite the ECML'14 paper and so we failed to find the ECML'14 paper and hence failed to cite it. Now, as to the actual contributions, unfortunately, the ECML'14 paper seems to have many problems. We believe that the stated results in it are incorrect: They miss essential conditions, after which their result would not be different from ours. This can be seen by looking at the arxiv version which has the proofs. The factual errors are many: for the expected error bound, on page 16 in Section A.2, the derivation up to (26) is correct. However, then it is claimed that \bar A_n - (1-beta) mu I is positive definite, which is easily seen to be false, e.g, when n = 1 (in which case \bar A_n is rank 1, asymmetric, has little to do with mu, the minimum eigenvalue of the empirical feature covariance matrix). This is not a minor problem: Fixing the proof requires arguments similar to ours. The high-probability bound is similarly incorrect: On page 14, the display in the bottom of the page is just incorrect. Again, fixing the proof requires extra conditions, leading to an analysis similar to ours. As a sidenote we remark that even the ICML'15 seems to have problems: We have contacted the first author, Prashanth, who confirmed the problems.

2) Work of Bach and Moulines: We think this is a precursor to and is superseded by the papers by DeÃÅfossez and Bach [2] (see page 5) and that of Dieuleveut, Flammarion and Bach [3] (see page 2), which we cite and compare to (page 6, lines 226-241). In fact, we think that this comparison is truly enlightening as it brings out in what way the LMS update is a very special case of LSA.

2) Knowledge of minimum eigenvalue: Indeed, the stepsize dependence on a quantity that is most likely unknown in an application is highly problematic! Our intention if anything was to make this very clear. In fact, here is where our lower bound becomes crucial: We are showing that this is not a weakness of the analysis, but a weakness of the approach. We think that shining light on this dependence is in fact what makes the paper uniquely valuable (however unhappy one may be about this dependence!). We realize that their is a tendency in the community to prefer constructive and positive results, but we think that negative results like ours are also extremely useful: there is much to be learned from knowing what does not work (and why). Gaining this level of understanding is only possible by a careful theoretical analysis like ours. The importance of our result is further elevated by that the topic of RL/TD/.. is rapidly gaining popularity, and that numerous works attempt to transport results from supervised learning to this setting, overlooking the extra difficulties (e.g., see the papers by Prashanth et al.). Our paper may be valuable for both people solely interested in RL, but also for the fast growing community of researchers coming to RL with a supervised learning background.

3-4) Differences to Prashanth-Korda and Prashanth-Korda-Munos? Why is our bound better? Also: line 275

5) TD/GTD can of course be used with data-resampling, which will not involve Markov noise. Since the LSTD equation is not derived from a loss, we are puzzled about the comment about SGD. Mirror-prox (=GTD) comes closest to what we think SGD may look like in this setting.

Difference to state of the art: See above.
Upper bound no guidelines: Lower bound shows there will be no guidelines. The adaptive algorithm is meant to address this, but further work is needed on this for sure. We think even without this further work, the work should be interesting for the community who cares about stochastic-approximation in the RL setting.

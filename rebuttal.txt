We thank the reviewers for their comments.
Reviewer 1:
Naturally, we are disappointed by the negative recommendation, which we think is perhaps the result of overlooking some facts:
1) Two justifications for the iid setting: The noise is iid in the data-resampling setting, which is quite common in RL [4,10,15,16]. Understanding the differences between the standard supervised setting and the RL setting should start with a setting with the least number of differences. We see that already the iid noise setting presents surprises (see below)
2) Novelties are many:
i) Analysis of general LSA schemes with multiplicative noise for constant stepsize combined with PR averaging is new in literature. Prior results were known only for general LSA with additive noise and for SGD (a special case). New analysis required significant departure from the prior work and produces different results.
ii) Matrices in general LSA schemes lack symmetry, hence prior results do not apply directly; the similarity transformation that we employ in the paper is needed to handle the non-symmetric case; this is a major step
iii) We show a negative result: for general LSAs the stepsize cannot be chosen in a problem independent manner; we provide a lower bound to show that the negative result is not due to a weakness in our analysis
iv) Our results give better than previous rates for GTD. This way, our paper obviates a highly problematic projection step in GTD introduced by[10]; this is a major gain. The projection step is problematic because the range of dual variables cannot be known a priori in GTD.
We would like to mention that we have discussed the significance of our contributions in the introduction as well as section 6. Hence, we think that the negative position is unwarranted and lacks justification.

Reviewer 2:
We will be happy to push the adaptive stepsize section to the appendix and provide a proof sketch in the main body.

Reviewer 3:
1) Missing citation to Prashanth, Korda and Munos' ECML'14 paper.
In fact, Prashanth and Korda later published arxiv.org/pdf/1411.3224.pdf (ICML'15), which we thought was the "state-of-the-art" and we cited this paper. Strangely, this paper does not cite the ECML'14 paper and so we failed to find the ECML'14 paper and hence failed to cite it. Now, as to the actual contributions, unfortunately, the ECML'14 paper seems to have many problems. In fact, a report by Lakshmi Narayanan and Szepesvári available at https://goo.gl/q9A4vn describes some of these problems. The factual errors are severe: for the expected error bound, on page 16 in Section A.2 of the arxiv version, after Eq. (26) it is claimed that \bar A_n - (1-beta) mu I is positive definite. This is easily seen to be false, e.g, when n = 1, in which case \bar A_n is rank 1, asymmetric and has nothing to do with mu, the minimum eigenvalue of the empirical feature covariance matrix. This is not a minor problem: Fixing the proof requires arguments similar to ours. The high-probability bound is similarly incorrect: On page 14, the display in the bottom of the page is just incorrect. Again, fixing the proof requires extra conditions, leading to an analysis similar to ours. As a sidenote, per the report above, it appears that the ICML'15 paper seems to have similar problems.

2) Work of Bach and Moulines: We think this is a precursor to and is superseded by the papers by Défossez and Bach [2] (see page 5 in [2]) and that of Dieuleveut, Flammarion and Bach [3] (see page 2 in [3]), which we cite and compare to (page 6, lines 226-241). In fact, we think that this comparison is truly enlightening as it brings out in what way the LMS update is a uniquely special case of LSA.

2) Knowledge of minimum eigenvalue: Indeed, the stepsize dependence on a quantity that is most likely unknown in an application is highly problematic! Our intention if anything was to make this very clear. In fact, here is where our lower bound becomes crucial: We are showing that the dependence is not an artifact of the analysis, but a weakness of the approach. Shining light on this dependence makes the paper uniquely valuable. We realize that there is a tendency in the community to prefer constructive and positive results, but we think that negative results like ours are also extremely useful: there is much to be learned from knowing what does not work (and why). Gaining this level of understanding is only possible by a careful theoretical analysis like ours. The importance of our result is further elevated by that the topic of RL/TD/.. is rapidly gaining popularity, and that numerous works attempt to transport results from supervised learning to this setting, overlooking the extra difficulties (e.g., see papers by Prashanth et al.). Our paper may be valuable for both people solely interested in RL, but also for the fast growing community of researchers coming to RL with a supervised learning background.

3) l.266 should have cited Theorem 2 [6] (PR avg case) which has no bias/variance decomposition (see $K_1^{IA}(n)$).

4) In l. 275 we cite [11], (page 4, $\alpha=1$); point is both $\mu c$ or $\mu c/2$ in the exponent are undesirable.

5) TD/GTD can of course be used with data-resampling, which will not involve Markov noise (as recognized by many earlier, e.g., [4,10,15,16]). Since the LSTD equation is not derived from a loss, we are puzzled about the comment about SGD. Mirror-prox (=GTD) comes closest to what we think SGD may look like in this setting.

Difference to state of the art: See above.
Upper bound gives no guideline: Lower bound shows there will be no guidelines; an adaptive algorithm is necessary (contrasting LMS). Our adaptive algorithm is meant to address this, but further work is needed on this for sure. We think even without this further work, the work should be interesting for the community as outlined above (see also the response to Reviewer 1).

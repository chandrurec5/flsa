%!TEX root =  flsa.tex
\section{Related Work}\label{sec:related}
We first discuss the related work outside of RL setting, followed by related work in the RL setting. In both cases, we highlight the new insights that follows from the results in this paper.\par
\paragraph{SGD for Linear Regression} As mentioned in \Cref{sec:intro}, we were motivated by the ``remarkable'' results obtained by \citet{bach}, where the authors analyze the performance of SGD for the problem of linear regression with quadratic loss. The problem class considered by them belongs to $\P_{\text{PSD},B}$ (see \Cref{def:pclass} ) and hence is weakly admissible with a uniform step-size choice. In order to produce a rate of $\frac{C}{t}$ (where $C>0$ is a problem independent constant) for the variance term \citet{bach} do not rely on admissibility, but instead use the fact that the MSE, $\EE{\norm{\thh_t-\ts}^2_{A_P}}$ for the linear prediction problem is measured in the general quadratic norm with respect to the symmetric positive definite matrix $A_P$ and the fact that the noise is `structured' i.e., the variance is bounded by $R\,A_P$ for some constant $R>0$. At the same time, the bias term decays at a rate $\frac{C'}{t^2}$, where the constant $C'$ contains the factor $(\theta_0-\ts)^\top A^{-1}_P (\theta_0-\ts)$ and is dependent on the condition number.
\paragraph{Our Results:} Analysis of LSA with CS-PR goes back to the work by \citet{polyak-judisky}, wherein they considered the additive noise setting i.e., $A_t=A$ for some deterministic Hurwitz matrix $A\in \R^{\dcd}$.
A key improvement in our paper is that we consider the `multiplicative' noise case, i.e., $A_t$ is a random matrix. To tackle the multiplicative noise we use newer analysis introduced by \citet{bach}. However, since the general LSA setting (with Hurwitz assumption) does not enjoy special structures of the SGD setting of \citet{bach}, we make use of Jordan decomposition and similarity transformations in a critical way to prove our results.
\paragraph{Notation for RL:} We are presented with data in the form of an \iid sequence $(\phi_t,\phi'_t,r_t)\in \R^d\times\R^d\times \R$. For a fixed constant $\gamma \in (0,1)$ define  $\Delta_t\eqdef \phi_t\phi_t^\top-\gamma \phi_t\phi_t^{'\top}$, $C_t=\phi_t\phi_t^\top$ and $b_t=\phi_r r_t$. In what follows, $\mu_t>0$ is an \emph{importance} sampling factor whose aim is to correct for mismatch in the (behavior) distribution with which the data was collected and the (target) distribution with respect to which one wants to learn. A factor $\mu_t=1,\,\forall t\geq 0$ will mean that no correction is required\footnote{This is known as the \emph{on-policy} case where the behavior is identical to the target. The general setting where $\mu_t>0$ is known as \emph{off-policy}.}. The various TD class of algorithms that can be cast as LSAs are given in \Cref{tb:tdalgo}
\FloatBarrier
\begin{table}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|l|p{60mm}|}\hline
Algorithm& Update & Remark\\ \hline
TD(0)& $\begin{aligned}\theta_t&=\theta_{t-1}+\alpha_t (b_t -\Delta_t\theta_{t-1})\end{aligned}$ & \cite{korda-prashanth}:$\alpha_t=O(\frac{1}{t})^\beta,\beta\in(0,1)$; PR-avg; $\EE{\norm{\eh_t}}= O(\frac{1}{\sqrt{t}}).$ \\ \hline
GTD/GTD2& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}- Q_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t A^\top_t y_{t-1})\end{aligned}$
& \cite{gtd2}: $\beta_t=\eta \alpha_t,\sum_{t\geq 0}\alpha_t=\infty, \sum_{t\geq 0}\alpha^2_t<\infty$; $e_t\ra 0$ as $t\ra\infty$ w.p.1. \cite{gtdmp}:
$\alpha_t=\beta_t=O(\frac{1}{\sqrt{t}})$; Projection+PR; $\,\norm{e_t} =O(\frac{1}{{t}^{\frac{1}{4}}})$ w.h.p.\\\hline
TDC& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}-C_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t b_t-\mu_t\Delta_t\theta_{t-1}+ (\Delta^\top_t-C_t)y_{t-1})\end{aligned}$ & \cite{gtd2}: $\beta_t=\eta \alpha_t,\sum_{t\geq 0}\alpha_t=\infty, \sum_{t\geq 0}\alpha^2_t<\infty$; $e_t\ra 0$ as $t\ra\infty$ w.p.1. \\\hline
\end{tabular}
}
\label{tb:tdalgo}
\caption{Some popular TD algorithms with rates known in literature \cite{korda-prashanth,gtd2,gtdmp}. }
\end{table}
\begin{comment}
\begin{table}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}\hline
Algorithm& Update &Stepsize &Rate \\ \hline
TD(0)& $\begin{aligned}\theta_t&=\theta_{t-1}+\alpha_t (b_t -\Delta_t\theta_{t-1})\\ \thh_t&=\frac{1}{t+1}\sum_{s=0}^t \theta_t\end{aligned}$ &$\begin{aligned}\alpha_t&=O(\frac1{t})^\delta,\\ &\delta\in(1/2,1)\end{aligned}$ &$\EE{\norm{\eh_t}}\leq O(\frac{1}{\sqrt{t}})$ for $\delta\ra 1$ \\ \hline
GTD& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}- Q_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t A^\top_t y_{t-1})\end{aligned}$ & $\begin{aligned}&\beta_t=\eta \alpha_t,\sum_{t\geq 0}\alpha_t=\infty, \\ &\sum_{t\geq 0}\alpha^2_t<\infty \end{aligned}$ & $e_t\ra 0$ w.p.1 \\\hline
TDC& $\begin{aligned} y_{t}&=y_{t-1}+\beta_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}-C_t y_{t-1})\\ \theta_t &=\theta_{t-1}+\alpha_t (\mu_t b_t-\mu_t\Delta_t\theta_{t-1}+ (\Delta^\top_t-C_t)y_{t-1})\end{aligned}$ & $\begin{aligned}&\beta_t,\alpha_t,\frac{\alpha_t}{\beta_t} \ra 0,~\text{as}~t\ra \infty,\\& \sum_{t\geq 0}\alpha_t=\infty, \sum_{t\geq 0}\beta_t=\infty, \\ &\sum_{t\geq 0}\alpha^2_t<\infty, \sum_{t\geq 0}\beta^2_t<\infty \end{aligned}$ & $e_t\ra 0$ w.p.1 \\\hline
GTD& $\begin{aligned} y_{t}&=\Pi_Y \left(y_{t-1}+\alpha_t(\mu_t b_t -\mu_t \Delta_t \theta_{t-1}-Q_t y_{t-1})\right)\\ \theta_t &=\Pi_{\Theta}\left(\theta_{t-1}+\alpha_t (\mu_t A_t^\top y_{t-1})\right)\\ \thh_t&=\frac{\sum_{s=0}^t \alpha_s\theta_s}{\sum_{s=0}^t \alpha_s}\end{aligned}$ & $\alpha_t =O(\frac{1}{\sqrt{t}})$ & $\norm{A_P\eh_t-b}^2 \leq O(\frac{1}{\sqrt{t}})$ w.h.p\\\hline

\end{tabular}
}
\label{tb:rates}
\end{table}
\end{comment}
\paragraph{Related Work in RL:} The \emph{temporal difference} learning algorithm or simply known as TD(0), is the most basic of the class of TD algorithms. An important shortcoming of TD(0) was its instability in the \emph{off-policy} case, and was successfully mitigated by the \emph{gradient temporal difference} learning GTD algorithm\cite{gtd2}. The GTD was first proposed by \citet{gtd} and its variants namely GTD2 and TDC were proposed later by \citet{gtd2}. However, the initial convergence analysis for GTD/GTD2/TDC were only asymptotic in nature \cite{gtd,gtd2} with diminishing step-sizes.\par
The most relevant to our results are those by \citet{korda-prashanth} in TD(0) and by \citet{gtdmp} in GTD. For the TD(0) case,  diminishing step-sizes $\alpha_t=O(\frac{1}{t})^\beta,\beta \in(0,1)$ with PR averaging is showed to exhibit a rate of $O(\frac{1}{t})$ decay for the MSE when $\beta\ra 1$ \cite{korda-prashanth}. In the case of GTD/GTD2 diminishing step-sizes $\alpha_t=O(\frac{1}{\sqrt{t}})$, projection of iterates and PR-averaging leads to a rate of $O(\frac{1}{\sqrt{t}})$ is achieved for the prediction error $\norm{A_P\thh_t-b_P}^2$ with high probability \cite{gtdmp}. \citet{gtdmp} also suggest a new version of GTD based on stochastic mirror prox ideas, called the GTD-Mirror-Prox which also achieves $O(\frac{1}{\sqrt{t}})$ rate for $\norm{A_P\thh_t-b_P}^2$ with high probability under similar step-size choice that was used by them for the GTD.
\paragraph{Our Results} \Cref{th:rate} applies to TD(0)/TDC/GTD/GTD2/GTDMP with CS-PR since all the TD class of algorithms satisfy the Hurwitz assumption. Finite time performance of TDC was unkown in prior literature and our work provides a first step by understanding TDC with CS-PR. Further, our results show that the error in the GTD class of algorithms decay at $O(\frac{1}{t})$ rate (even without use of projection or mirror maps) instead of $O(\frac{1}{\sqrt{t}})$,  which is an improvement. In comparison to the TD(0) results by \citet{korda-prashanth}, \Cref{th:rate} is better in that it provides the bias/variance decomposition. Further, their results are specific to the TD(0) where $A_P$ is known to be positive definite, and hence cannot be extended in a straightforward manner to GTD class or TDC.
However, it is important to note that \citet{korda-prashanth} handle the Markov noise case which is not dealt with in this paper.
\begin{comment}
\paragraph{Related Work in RL} The various temporal difference learning algorithms can be written as in \Cref{table:td}. The TD(0) algorithm is an on-policy algorithm ($\mu_t=1$) and is unstable in the general {off-policy} setting. The gradient temporal difference (GTD) algorithm \cite{gtd,gtd2,gtdmp} is stable in off-policy setting as well. Recent work by \cite{korda-prashanth} derive finite time bounds for the TD(0) for two different cases of step-sizes one without PR averaging and other with PR averaging. In the case without PR averaging, \cite{korda-prashanth} consider step-sizes of the form $\frac{c_0c}{c+t}$, where $c_0$ and $c$ are problem dependent positive constants, to dervie a rate of $O(\frac{1}{t})$ for the MSE.  In the case with PR averaging (row $1$ of \Cref{tb:rates}), \cite{korda-prashanth} consider step-size of the from $c_0\left(\frac{c}{c+t}\right)^\delta,\,\delta \in (1/2,1)$ where $c_0, c$ are problem independent positive constants, and show that this choice yeilds a rate of $O(\frac{1}{t})$ for $\delta \ra 1$ and $t\geq t_0$ (where $t_0$ is a problem dependent time instant). A possible limiting feature of the results by \cite{korda-prashanth} is that either the problem knowledge is required or the bounds hold only after a problem dependent time. A centered TD(0) variant is also considered by \cite{korda-prashanth}, however, a projection operator is required to keep the iterates bounded. Also, the bounds by \cite{korda-prashanth} don't have an explicit bias variance decomposition. The works on GTD and TDC by \cite{gtd,gtd2} show only asymptotic convergence w.p.1. Recently, the work by \cite{gtdmp} shows a rate of $\frac{1}{\sqrt{t}}$ for GTD with use of projection operation, and also proposed a variant namely GTD-Mirror-prox based on stochastic mirror-prox ideas which achieves $\frac{1}{\sqrt{t}}$ rate without use of projection. In comparison, we don't use of projections, our bounds hold for all time $t$, and the bounds are dependent on the problem instance only to the extent of choosing a step-size that leads to stable behavior, in which case we show that $O\frac{1}{t}$ rate holds (the scaling factor hidden in the $O$ expression is problem dependent). Further, our results provide explicit bias variance decomposition.
\end{comment}

\begin{abstract}
We consider $d$ dimensional linear stochastic algorithms with constant step-size and averaging, also known as Polyak-Ruppert (PR) averaging) of iterates.  In most applications, the aim of the LSA is to compute some $\ts\in \R^d$ using noisy data and $O(d)$ updates per iteration. We are motivated by the problem policy evaluation from experience replay using the \emph{temporal difference} (TD) class of learning algorithms arising in reinforcement learning (RL) that are also LSA algorithms. Under a constant step-size, and PR averaging, we provide bounds for the mean squared error after of the LSA  after $t$ iterations. The restrictions we place are that the data has finite variance and that the expected dynamics of the LSA is Hurwitz. We observe that under the said assumptions the MSE error in LSA with constant step-size and PR averaging decays as $O(\frac{1}{t})$.\par
We also examine  the conditions under which the constant step-size can be chosen in a data independent fashion for a given LSA, and show that not all problem classes admit such a uniform constant step-size.
We also suggest a heuristic step-size tuning algorithm that choose a constant step-size for each given problem instance. We compare our results with related work and also discuss the implication of our results in the context of TD algorithms arising in RL.
\end{abstract}

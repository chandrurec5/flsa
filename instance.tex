%!TEX root =  flsa.tex
\section{Main Results and Discussion}\label{sec:mainresults}
In this section, we derive instance dependent bounds that are valid for a given problem $P$ (satisfying \Cref{assmp:lsa}) and in the \cref{sec:uniform}, we address the question of deriving uniform bounds $\forall\,P\in \P$, where $\P$ is a class of distributions (problems). Here, we only present the main results followed by a discussion. The detailed proofs can be found in \cref{sec:proofs}. 
\todoc{Usually it is expected that one gives an outline of the proof technique. Especially in a theory paper. At least verbally we could explain the tools used, how the proof is similar and/or different to previous proofs.}
In what follows, for the sake of brevity, we drop the subscript $P$ in the quantities $\EEP{\cdot}$, $\sigma^2_{A_P}$ and $\sigma^2_{b_P}$. \todoc{I dropped $P$ from $\EEP{\cdot}$ before. Either put there back, or remove $\EEP{\cdot}$ from here.}
%\begin{lemma}\label{lm:simtran}[Similarity Transformation]
%Let $P$ be Hurwitz, then there exists a $U\in \gln$ such that $P_U$ is positive definite.
%\end{lemma}
We start with a lemma, which is needed to meaningfully state our main result:
\begin{lemma}\label{lm:hur}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}. \todoc{Is it important that the distribution is over real-valued stuff? Would the results work for complexed-valued data? I would think so.}
Then there exists an $\alpha_{P_U}>0$ and $U\in \gln$ such that $\rhod{P_U}>0$ and $\rhos{P_U}>0$
holds for all $\alpha \in (0,\alpha_{P_U})$. 
\end{lemma}
\begin{comment}
\begin{proof}
\begin{align*}
\rhos{P}&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^*\EE{A_t^* A_t} x\\
&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^* A^*_P A_P -\alpha x^* \EE{M_t^* M_t} x\\
&\geq \lambda_{\min}(A^*_P+A_P)-\alpha \norm{A_P}^2-\sigma^2_P
\end{align*}
The proof is complete by choosing $\alpha_P<\frac{\lambda_{\min}(A^*_P+A_P)}{\norm{A_P}^2+\sigma^2_P}$
\end{proof}
\end{comment}


\begin{comment}
\begin{theorem}\label{th:pdrate}
Let $P^A$ in \Cref{assmp:lsa}-~\ref{dist} be a positive definite distribution over $\C^{\dcd}$. Then
\begin{align*}
\EE{\norm{\thh_t-\ts}^2}
\leq
\left(1+\frac2{\alpha\rhod{P}}\right)\frac{1}{\alpha \rhos{P}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\theta_0-\ts})}{t+1} \right)\,.
\end{align*}
\end{theorem}

\begin{theorem}\label{thm:simtran}[Change of Basis]
Let $P$ in \Cref{assmp:lsa}-~\ref{dist} be AS. Define $\gamma_t\eqdef U^{-1}\theta_t,\,,\gamma^*\eqdef U^{-1}\ts$, and suppose if $P_U$ is positive definite, then
\begin{align*}
\EE{\norm{\gh_t-\gamma^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U^{-1}}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\gamma_0-\gamma^*}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\gamma_0-\gamma^*})}{t+1} \right)\,,
\end{align*}
where $\gh_t=\frac{1}{t+1}\sum_{s=0}^t \gamma_s$.
\end{theorem}
\end{comment}

\begin{theorem}\label{th:rate}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}.
Then, for  $U\in\gln$ and $\alpha_{P_U}>0$ as in \cref{lm:hur},
\todoc{Why not use $\alpha_P$? $U$ is dependent on $P$. If $\alpha_{P_U}$ does not have a specific definition, there is no point to denote this dependence on $U$. It just adds to the clutter.}
for all $\alpha\in (0,\alpha_{P_U})$ and for all $t\ge 0$,
\begin{align*}
\EE{\normsm{\thh_t-\ts}^2}
\leq
\nu\,
\left\{\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{v^2}{t+1} \right\}\,,
\end{align*}
where $\nu = \left(1+\tfrac2{\alpha\rhod{P_U}}\right)\tfrac{\cond{U}^2}{\alpha \rhos{P_U}}$ and
$v^2 = 
\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}$.
\end{theorem}
Note that $\nu$ depends on $P_U$ and $\alpha$, while $v^2$ in addition also depends on $\theta_0$. The dependence,  when it is essential, will be shown as a subscript.
\begin{theorem}[Lower Bound]\label{th:lb}
There exists a distribution $P$ over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}, such that
there exists $\alpha_P>0$ so that $\rhos{P}>0$ and $\rhod{P}>0$ hold for all $\alpha\in (0,\alpha_P)$ and
for any $t\ge 0$, \todoc{$t\ge 1$?}
\begin{align*}
\EE{\normsm{\thh_t-\ts}^2} 
&\geq \frac{1}{\alpha^2 \, \rhod{P}\rhos{P}} \,\left\{ \frac{\beta_t \norm{\theta_0-\ts}^2}{(t+1)^2} 
+ \frac{v^2\sum_{s=1}^t \beta_{t-s}  }{(t+1)^2} \right\}\,,
%&\geq \frac{1}{(t+1)^2}(\alpha)^{-2}(\rhod{P}\rhos{P})^{-1}\Big\{ \big(1-(1-\alpha \rhos{P})^t\big) \norm{\theta_0-\ts}^2 \\
%&+ \sum_{s=1}^t \big(1-(1-\alpha \rhos{P})^{(t-s)}\big) \big(\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}\big) \Big\}\,.
\end{align*}
where $\beta_{t} =  \big(1-(1-\alpha \rhos{P})^t\big)$ and $v^2$ is as in \cref{th:rate}.
\begin{comment}
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}
\end{comment}
\end{theorem}
Note that $\beta_t \to 1$ as $t\to\infty$. Hence, the lower bound essentially matches the upper bound.
\begin{comment}

Consider the LSA with $b_t=0$, and $P^0$ has all the mass concentrated on $A_{P^0}=\begin{bmatrix} \lambda_{\min} &0\\ 0& \lambda_{\max}\end{bmatrix}$, for some $\lambda_{\max}>\lambda_{\min}>0$. Note that in this example $\ts=0$.
By choosing $\alpha<\frac2{\lambda_{\max}}$, in this case it is straightforward to write the expression for $\eh_t$ explicitly as below:
\begin{align*}
\eh_t&=\frac{1}{t+1}\sum_{s=0}^t e_t = \frac{1}{t+1}\sum_{s=0}^t (I-\alpha A_P)^{t-s} e_0\\
&=\frac{1}{t+1}(\alpha A)^{-1}\left(I-(I-\alpha A)^{t+1}\right)e_0
\end{align*}

\begin{align*}
\norm{\eh_t}^2&=\frac{1}{(t+1)^2}\norm{(\alpha A_P)^{-1}\left(I-(I-\alpha A_P)^{t+1}\right)e_0}^2
&\leq\frac{1}{(t+1)^2}\alpha^2\norm{A_P^{-1}}^2\norm{e_0}^2
\end{align*}

\begin{align*}
\norm{\eh_t}^2\geq \eh^2_t(1)=\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)
\end{align*}
Note that in this example, $\rhos{P^0}=\rhod{P^0}=\lambda_{\min} -\alpha \lambda_{\min}^2=\lambda_{\min}(1-\alpha \lambda_{\min})$. Sandwiching the error between the lower and upper bounds we have
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}
\end{comment}
In what follows, we discuss the specific details of these results. 

%\vspace*{0.5em}
%\subsection{Result Discussion}
%\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\textbf{Role of $U$}: When the given problem $P$ is positive definite, it is straightforward to produce an $\alpha_P>0$ such that $\rhos{P}>0$ and $\rhod{P}>0,\,\forall \alpha\in(0,\alpha_P)$ by considering the definitions of $\rhos{P}$ and $\rhod{P}$. 
However, the problem $P$ being Hurwitz does not guarantee positive definiteness. % (see \Cref{ex:pdas}). 
\todoc{I commented out the examples.}
Following  \cite{lihong}, we remedy this 
by applying a similarity transformation using an appropriate $U$ that transforms a Hurwitz problem $P$ to a positive definite problem $P_U$. This can be done because any Hurwitz matrix is similar to a PD matrix. 
Taking $U$ to be this similarity matrix, we transform  \eqref{conststep} by premultiplying it by $U^{-1}$ and introducing a new variable $\gamma_t\eqdef U^{-1 }\theta_t$.
\if0
To see how this works, let $\gamma_t\eqdef U^{-1 }\theta_t$. Premultiplying 
 \eqref{conststep} by $U^{-1}$ and plugging in $\gamma_t$, we get
\begin{align*}
%U^{-1}\theta_t&=U^{-1}\theta_{t-1}+\alpha(U^{-1} b_t- U^{-1}A_t U U^{-1}\theta_{t-1}),\\
\gamma_t&=\gamma_{t-1}+\alpha(U^{-1} b_t- J_t\gamma_{t-1}),
\end{align*}
where $J_t = U^{-1}A_t U$.
Now, since any Hurwitz matrix is similar to a PD matrix with an appropriate similarity transformation $U$, choosing $U$ this way we get that $\E[J_t]$ is PD, hence $P_U$ is PD.
\fi
%This completes the transformation of a Hurwitz problem $P$ to a positive definite problem $P_U$. 
The condition number $\cond{U}$ appears in our bound because $\theta_t$ is recovered from $\gamma_t$ using
$\theta_t=U\gamma_t$. Of course, when the problem $P$ is positive definite, we can let $U=\I$ in \Cref{th:rate}. 
More generally, one can always take $U$ in the result that leads to the smallest bound.

\textbf{Role of $\rhos{P}$ and $\rhod{P}$}: 
When $P$ is positive definite, we can expand the MSE as 
\begin{align}
\label{eq:exp}
	\EE{\norm{\eh_t}^2}=\tfrac{1}{(t+1)^2}\, \ip{ \textstyle\sum_{s=0}^t e_s,\textstyle\sum_{s=0}^t e_s}\,,
\end{align} 
where $\eh_t = \thh_t-\ts$ and $e_t = \theta_t-\ts$.
The inner product in \eqref{eq:exp} is a summation of \emph{diagonal} terms $\EE{\ip{e_s,e_s}}$ and \emph{cross} terms of $\EE{\ip{e_s,e_q}}$, $s\neq q$. The growth of the diagonal terms and the cross terms depends on the spectral norm of the random matrices $H_t=I-\alpha A_t$ and that of the deterministic matrix $H_P=I-\alpha A_P$, respectively. These are given by
\begin{comment}
The output $\thh_t$ of the algorithm \eqref{eq:lsa} is the average of the internal states at times $s=0,\ldots,t-1$. The error dynamics of the internal states by looking at the behavior of $e_t\eqdef \theta_t-\ts$.
\begin{align}\label{eq:errec}
\theta_t&=\theta_{t-1}+\alpha(b_t-A_t\theta_{t-1})\nn\\
\theta_t-\ts&=\theta_{t-1}-\ts+\alpha\left(b_t-A_t(\theta_{t-1}-\ts+\ts)\right)\nn\\
e_t&=e_{t-1}+\alpha(b_t-A_t e_t -A_t\ts)\nn\\
e_t&=\underbrace{((I-\alpha A_t))}_{\text{Random-Matrix}} e_{t-1}+\underbrace{\alpha(N_t -(M_t)\ts)}_{\text{Noise}}
\end{align}
From \eqref{errec} it is clear that the error dynamics depends on the $i)$ properties of the random matrix, $ii)$ properties of the noise. In the absence of the noise term in \eqref{eq:errec}, we have $e_t=(I-\alpha A_t) e_{t-1}=\Pi_{s=1}(I-\alpha A_s) e_0$, i.e., the initial error is get multiplied by a product of random matrices. In such a scenario, we can guess that whether or not the algorithm forgets the bias $\norm{\theta_0-\ts}$, depends on whether the matrix product is contracting. One way to characterize this contracting property is to look at the spectral radius of the random matrix, which is given by
\end{comment}
\begin{align}\label{eq:spectralrand}
\EE{\norm{I-\alpha A_t}^2}=1-\alpha \rhos{P}, \quad  {\norm{I-\alpha A_P}^2}=1-\alpha \rhod{P}\,,
\end{align}
justifying the appearance of $\rhos{P}$ and $\rhod{P}$.
%Note that \eqref{eq:spectralrand} explicitly connects $\rhod{P}$ and $\rhos{P}$ in \Cref{def:dist} to the spectral norms matrices $H_t$ and $H_P$. 
For the MSE to be bounded, we need the spectral norms to be less than unity, implying the conditions $\rhos{P}>0$ and $\rhod{P}>0$. If $P$ is Hurwitz, we can argue on similar lines by first transforming $P$ into a positive definite problem $P_U$ and replacing $\rhos{P}$ and $\rhod{P}$ by $\rhos{P_U}$ and $\rhod{P_U}$, and introducing $\cond{U}$ to account for the forward ($\gamma=U^{-1}\theta$) and reverse ($\theta=U\gamma$) transformations using $U^{-1}$ and $U$ respectively.

\textbf{Constants} {$\alpha$, $\rhos{P}$ and $\rhod{P}$} do not affect the exponents $\frac{1}{t}$ for variance and $\frac{1}{t^2}$ for bias terms.%
\todoc{Why is this a great thing? Refer to other works etc.}

\textbf{Bias and Variance}: The MSE at time $t$ is bounded by a sum of two terms. The first \emph{bias} term is given by $\B=\nu_{P_U,\alpha} \,\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}$, bounding how fast the initial error $\norm{\theta_0-\ts}^2$ is forgotten. 
The second \emph{variance} term is  given by $\V=\nu_{P_U,\alpha}\, \frac{\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} $ and captures the rate at which noise is rejected.

\textbf{Behaviour for extreme values of $\alpha$}: 
As $\alpha\to 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ there. This is unavoidable (see also \cref{th:lb}) and is due to the slow forgetting of initial conditions for small $\alpha$. Small step-sizes are however useful to suppress noise, as seen from that in our bound $\alpha$ is seen to multiply the variances $\sigma^2_A$ and $\sigma^2_b$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms are trading off the two types of errors. 
For larger values of $\alpha$ with $\alpha_P$ chosen so that $\rhos{P}\ra 0$ as $\alpha\ra \alpha_{P}$  (or $\alpha_{P_U}$ as the case may be), the bounds blow up again.

\textbf{The lower bound} of \Cref{th:lb} shows that the upper bound of \cref{th:rate} is tight in a number of ways.
In particular, the coefficients of both the $1/t$ and $1/t^2$ terms inside $\{ \cdot \}$ are essentially matched.
Further, we also see that
the $(\rhos{P}\rhod{P})^{-1}$ appearing in $\nu_{P_u,\alpha}$ cannot be removed from the upper bound. 
Note however that there are specific examples, such as SGD for linear least-squares,
where this latter factor can in fact be avoided (for further remarks see \Cref{sec:related}).

\input{illustrate}

\section{Introduction}
\begin{comment}
Stochastic Approximation (SA) algorithms can be expressed in the general form
\begin{align}\label{eq:sa}
\theta_t=\theta_{t-1}+\alpha_t f(\theta_{t-1},\zeta_t),
\end{align}
where $\theta_t\in \R^d$ are the iterates, and $f(\theta_{t-1},\zeta_t)$ of the function $g(\theta_{t-1})=\EE{f(\theta_{t-1})}_{\zeta_t}$, where the expectation is with respect to some noise process $\zeta_t$. At each time step, the algorithm updates the iterates in increments proportional to a step-size $\alpha_t>0$ and the idea is to eventually converge to $\ts\in \R^d$ which is a desired optimum or a fixed point. Due to their inherent online nature SA algorithms appear widely in machine learning applications \cite{konda,bhatnagar,borkar,gtd,td,bach}, where we have to learn from a noisy data stream.\par
The choice of the step-size $\alpha_t$ needs to trade-off learning rate versus filtering noise, so to speak. In general, one can guess that larger step-size lead to faster learning and at the same time smaller step-sizes are better to handle noise. A popular choice of step-size schedule that is known handle the noise $\alpha_t=O(\frac{1}{t})$. However, since the step size are diminishing, it can hurt the learning rate. An alternative suggestion is to use more aggressive step sizes without minding the noise in the iterates $\theta_t$ in \eqref{eq:sa}, however, introduce additional averaging (known as Polyak-Rupert averaging) to filter the noise. The averaged iterates can be given by $\th_t\eqdef\frac{1}{t}\sum_{s=0}^t \theta_t$, and an analysis of the PR-averaging technique is known from \cite{}.\par
\end{comment}

A linear stochastic approximation (LSA) algorithm is given by
\begin{align}\label{eq:lsaintro}
\theta_t=\theta_{t-1}+\alpha_t (b_t-A_t \theta_{t-1}),
\end{align}
where the iterates, $\theta_t\in \R^d$, and step-sizes, $\alpha_t>0$, are algorithmic quantities, and $(b_t,A_t)\in \R^d\times \R^{\dcd}$ is the noisy data sequence presented to the algorithm.
LSA algorithms are useful in machine learning and reinforcement learning (RL) applications, where the aim is to compute a $\ts\in \R^d$ using the noisy data sequence, using $O(d)$ updates per iteration. Some examples of LSAs include the stochastic gradient descent algorithm (SGD) for the problem of linear prediction with quadratic loss \cite{bach,bachaistats}, and the temporal difference class (TD) of learning algorithms in RL \cite{sutton,konda-tsitsiklis,gtd,gtd2,gtdmp}.\par
%An additional feature in these class of applications is that $A_t$ turns out to be a rank-$1$ matrix and $A_t\theta_{t-1}$ can be obtained in $O(d)$, which is attractive due to the cheap per time step computational requirement.\par
It is of interest to understand the effect of the step-sizes $\alpha_t$ (a quantity that can be controlled by the algorithm designer) on the performance of the LSA in \eqref{eq:lsaintro}. The choice of the step-sizes dictates learning rate, noise rejection and stability. Informally speaking, smaller step-sizes are better for noise rejection and larger step-sizes lead to faster learning, while at the same time too large step-sizes might result in instability of \eqref{eq:lsaintro}. Diminishing and square-summable step-sizes have been a popular choice for the step-sizes, where $\alpha_t\ra 0$ such that $\sum_{t\geq 0} \alpha_t=\infty$, $\sum_{t\geq 0}\alpha^2_t<\infty$ \cite{gtd2,borkarbook,konda-tsitsiklis}. An alternative technique, which we call LSA with constant step-size Polyak-Ruppert (CS-PR) averaging, is to run \eqref{eq:lsaintro} as a constant step-size LSA by choosing $\alpha_t=\alpha>0,\forall t\geq 0$, and output the average $\thh_t\eqdef\frac{1}{t}\sum_{s=0}^t \theta_t$. In LSA with CS-PR, $\theta_t$ is an internal variable and $\thh_t$ is the output of the algorithm.
Averaging in stochastic approximation to improve noise rejection is a procedure that goes back to  Polyak and Ruppert \cite{polyak-judisky,ruppert}.
%In what follows, when we say CS-PR for LSA or LSA with CS-PR, we allude to the case where \eqref{eq:lsaintro} is a constant step-size LSA with $\alpha_t=\alpha>0,\forall t\geq 0$, $\theta_t$ in \eqref{eq:lsaintro} is considered an internal variable and $\thh_t$ is the final output of the LSA with CS-PR.
%In LSA with CS-PR, \eqref{eq:lsaintro} is a constant step-size LSA with $\alpha_t=\alpha>0,\forall t\geq 0$, $\theta_t$ in \eqref{eq:lsaintro} is considered an internal variable and $\thh_t$ is the final output of the LSA with CS-PR.
Intuitively, the CS-PR technique can be seen to address the learning vs noise rejection trade-off, where the constant step-size takes care of the learning  and the averaging of the iterates takes care of the noise rejection. \par
Recently, \cite{bach} considered SGD\footnote{SGD is a special of the LSA algorithm in \eqref{eq:lsaintro}.} with CS-PR for the linear prediction problem with quadratic loss and $i.i.d$ sampling. A ``remarkable'' result by \cite{bach} is that there exists a constant step-size such that for any problem where the noisy data is bounded by a known constant, the expected squared prediction error after $t$ updates is at most $\frac{C}{t}$ with a constant $C>0$ that depends \emph{only} on the bound on the data.\par\todoc{Dimension?}
\paragraph{Motivation and Contribution:} In this paper, we wish to study LSA with CS-PR for $i.i.d$ data with bounded variance. An additional restriction we place is that the matrix $\EEP{A_t}$ (where $A_t\sim P$ ) is Hurwitz, i.e., all its eigenvalues have positive real parts. For a given $P$, we wish to measure the performance in terms of the mean square error (MSE) $\EEP{\norm{\thh_t-\ts}^2}$. Further, for a given class of distributions $\P$, we also would like to know whether it is possible to ensure uniform performance $\forall P\in \P$.
Our study is motivated by the problem \emph{policy evaluation} \cite{dann} using linear value function approximation from experience replay \cite{lin} in a batch setting \cite{lange} in RL using TD class \cite{sutton,konda-tsitsiklis,gtd,gtd2,gtdmp} of algorithms. We now list our contributions in this paper
%While asymptotically diminishing step-sizes are common in RL literature \cite{ilstd,gtdmp,korda-prashanth}, the constant step-size choice has also been used in the past \cite{gtd2}.
\begin{itemize}[leftmargin=*]%, before = \leavevmode\vspace{-\baselineskip}]
\item \textbf{Instance Dependent Bounds} We show that for a fixed $P$, there exists a $\alpha_P>0$ such that the mean squared error (MSE) of the LSA with CS-PR is at most $O(\frac{1}{t})$, whenever $\alpha\in (0,\alpha_P)$. The MSE further can be broken down to a \emph{bias} term (arising due to the initial condition $\theta_0$) that decays at a rate $O(\frac{1}{t^2})$ and a \emph{variance} term (due to the noise) that decays at at rate $O(\frac{1}{t})$.
\item \textbf{Uniform Step-Sizes:} We also show (via counter example) that there is a class $\P$ that does not `admit' a constant step-size choice that guarantees uniform performance $\forall P\in \P$. We also argue that the subclass $\P_{LR}$ related to SGD for linear regression does `admit' a problem independent constant step size, thereby making a connection between our framework and that considered by \cite{bach}.
\item \textbf{Automatic Step-Size:} We argue informally that for any given $P$, a constant step-size can be chosen in an `automated' fashion, and the performance of CS-PR is comparable to other popular step-size choices used for LSA in RL literature.
\end{itemize}
\begin{comment}
\paragraph{Highlights:}
The performance of CS-PR for general LSA has been previously studied under additive noise assumptions where $A_t=A$ for some fixed $A\in\R^{\dcd}$ and the noise is present only in $b_t$ \cite{polyak-judisky}. The setting we consider in this paper is known as the multiplicative noise setting, whose analysis requires tools that exploit the linear structure of the LSA \cite{bach}. However,
 in the absence of special structures (such as symmetry of $A_t$),  we make use of Jordan/Cyclic decomposition in a critical way to handle the general Hurwitz assumption. Our results are applicable to wide variety of TD algorithms such as TD(0), GTD/GDT2, TDC. Further, the error dynamics of the CS-PR technique for LSAs has a simple interpretation in terms of behaviour of the sum of terms in a geometric series, and hence offers a simple and unified understanding of its use in the various TD algorithms.
An important difference between the linear prediction problem considered by \cite{bach} and the general LSA setting we consider is that in the former has certain special structures and the latter does not. One of the special structure exploited by \cite{bach} is the fact that $A_t$ are real symmetric matrices. The general LSA setting does not enjoy such an additional structure presents an interesting challenge. Further, CS-PR for general LSAs has been studied previous only under the additive noise setting \cite{polyal-judisky}.\par
\end{comment}
\paragraph{Organization:}
\begin{comment}
A host of algorithms in machine learning and in particular, the temporal difference class of algorithms arising in reinforcement learning, turn out to be special cases of \eqref{eq:sa}, namely linear stochastic approximation (LSA) algorithms. LSA algorithms assume the general form
 Further, in most instances of LSA $A_t$ turns out to be a rank-$1$ matrix and $A_t\theta_{t-1}$ can be computed in $O(n)$, and are desired for this low per iteration computational cost. Recently, \cite{bach} used \eqref{eq:lsaintro} with constant step-size, i.e., $\alpha_t=\alpha,\,\forall t\geq 0, \, \alpha >0$ and PR-averaging for the linear prediction problem with squared loss and $i.i.d$ sampling. A ``remarkable'' result by \cite{bach} is that there exists a constant step-size such that for any problem where the noisy data is bounded by a known constant, he expected squared prediction error after $t$ updates is at most $\frac{C}{t}$ with a constant $C>0$ that depends \emph{only} on the bound on the data.\todoc{Dimension?}

In particular, as opposed to many earlier results available in the literature \todoc{Add citations}, $C$ does \emph{not} depend on the conditioning of the underlying system, while the total runtime of the algorithm scales linearly with both $t$ and $n$.\par
In this paper, we want to understand the performance of constant step size with PR-average of iterates for general class of LSAs (outside of the linear prediction setting considered by \cite{bach}). We are interested in the problem of linear value function approximation in reinforcement learning using temporal difference learning (TD) either from experience replay in a batch setting or solving linear systems using TD-style algorithms.
\todoc{TD-style, or just TD?} \todoc{Later we need to comment on TD with eligibility traces and other variations of TD, e.g., new variants.} We now list our contributions, and its implications to the TD class of algorithms, and the key differences between the techniques employed here in comparison to \cite{bach}. We now list our contributions, the implications of our results, and the challenges presented by the general setting in comparison to the linear prediction setting considered by \cite{bach}.
\paragraph{Contributions}:
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item We show that in the case of general LSA, there exists instance dependent constant step size choice such that the mean squared error of the PR-average given by $\err_t=\EE{\norm{\thh_t-\ts}^2}$ is at most $O(\frac{1}{t})$. The error $\err_t$ further can be broken down to a \emph{bias} term (arising due to the initial condition $\theta_0$) that decays at a rate $O(\frac{1}{t^2})$ and a \emph{variance} term (due to the noise) that decays at at rate $O(\frac{1}{t})$.
\item We also show (via counter example) that not all classes of general LSAs `admit' a constant step-size that is uniform across all the problems in the class. Further, we also argue that the subclass of LSAs related to linear prediction problem does `admit' a problem independent constant step size.
\item The assumptions in our setting is comparable to those in the asymptotic analysis based on ordinary differential equation (ODE) method by \cite{borkarmeyn,borkarbook}.
\item Two timescale LSA algorithms are those that use two set of iterates (namely slower and faster) and two separate step-size schedules. Our results also hold for two timescale LSAs as long as the two different step-sizes are held constant with respect to time. Thus our setting not only holds for TD(0) but also extends to GTD/GTD2 and TDC.
\item On an intuitive level, the error dynamics of the constant step-size plus PR-averaging procedure is similar to the behavior of partial sums of a geometric series. This coupled with the fact that the result also encompasses two timescale LSAs presents a simple and unified view of the various TD algorithms.
\item The break up of error into bias and variance terms and their respective rates were previously worked out by \cite{bach} in the linear prediction setting, and here we extend the same to the general class of LSAs. Nevertheless, there are certain key differences between the general LSA setting that we consider to the setting considered by \cite{bach}, which are listed as under.
\end{itemize}
\end{comment}
\begin{comment}
\paragraph{Implications:}
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item Our results hold under the weak assumption\footnote{When this weak assumption fails to hold, the LSA is unstable and hence the notion of convergence is void.} that all the eigen values of $\EE{A_t}$ have strictly real parts. This assumption holds for all known TD class of RL algorithms.
\item Two timescale LSA algorithms are those that use two set of iterates (namely slower and faster) and two separate step-size schedules. Our results also hold for two timescale LSAs as long as the two different step-sizes are held constant with respect to time. Thus our setting not only holds for TD(0) but also extends to GTD/GTD2 and TDC. This provides a unified view of the various temporal difference learning algorithms.
\item The intuition behind the error analysis can be explained in simple terms when the noise free case is considered (i.e., $A_t=A$ in \eqref{eq:lsaintro}). In this case, the LSA is just a linear time invariant dynamical system where the error $\theta_t-\ts=(I-\alpha A)^t (\theta_0-\ts)$ and $\norm{\theta_t-\ts}=\rho^t\norm{\theta_0-\ts}$ (for some $0<\rho<1$). Now the PR-average is given by $\frac{\norm{\theta_t-\ts}^2}{t^2}=\frac{\sum_{s=0}^{t-1}\rho^s}{t^2} \norm{\theta_0-\ts}^2$. Note that the numerator is just sum of terms of a geometric series, which is summable as long as the matrix $(I-\alpha A)$ has eigen values within the unit circle.
\item The fact that the setting covers even two timescale LSAs with constant step size, coupled with the intuitive interpretation of the error dynamics together present a simple and unified view of the various temporal difference learning algorithms in literature.
\item One of the major bottlenecks in selecting step-size for general LSA is the lack of problem knowledge. One way to handle this issue is to choose a diminishing step size schedule such as $\alpha_t=\frac{1}{t}$. However, such a strategy can fail when the underlying matrices are ill-conditioned (see \Cref{sec:stepsizes}). Another popular approach has been to choose $\alpha_t=\frac{c}{c+t}$, where initially the step-sizes are held almost constant and then as $t$ increases they decay at rate $O(\frac{1}{t})$. While this approach can mitigate the ill-conditioning problem, it can at the same time amplify the effect of noise. In this paper, we argue informally that the constant step-size with PR-averaging of iterates can be a promising way to mitigate these issues, by periodically checking for possible blow up of the norm of the iterates and then reducing the step-size based on this information (see \Cref{sec:stepsizes}).
\end{itemize}
\paragraph{Challenges:} The break up of error into bias and variance terms and their respective rates were previously worked out by \cite{bach} in the linear prediction setting, and here we extend the same to the general class of LSAs. Nevertheless, there are certain key differences between the general LSA setting that we consider to the setting considered by \cite{bach}, which are listed as under.
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item In the linear least-squares problem is considered by \citet{bachharder} and the random matrices involved are known to be symmetric positive definite. This enables \citet{bachharder} to define operators that act between spaces of \emph{symmetric} matrices and carry out the computations making use of such operators. In our case, the matrices are known only to be (laxly) positive definite, i.e., they lack symmetry. \todoc{We should point out after our analysis in which step the symmetry helps \citet{bachharder}. We should also make this somehow clear here if possible.}
Thus, the above approach does not work, and instead we have resort to an analysis that makes use of only the expected norms of the random matrices involved.
\item Another significant difference is that in linear least-squares, the ``noise'' has a favorable ``structure''. Again, due to the lack of symmetry, this favorable structure does not apply. \todoc{Again, after the analysis, we should point out how the structured noise would have helped. I am wondering whether a more graceful degradation should happen? Is this really all or nothing?}
\end{itemize}
\end{comment}

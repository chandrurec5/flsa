\documentclass{article}
\input{pack.tex}
% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{varioref}
%\usepackage{thmtools}
\usepackage{aliascnt}
\usepackage{titlecaps,cleveref}
\usepackage{amsmath,amssymb,graphicx}
\crefname{assumption}{Assumption}{Assumption}

\title{Finite Time Analysis of Linear Stochastic Approximation}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
%  David S.~Hippocampus\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\begin{abstract}

\end{abstract}

\section{Introduction}


\section{Notations and Definitions}

We denote the sets real and complex number by $\R$ and $\C$ respectively. For $x=a+ib\in \C$, we denote its real and imaginary parts by $\re{x}=a$ and $\im{x}=b$ respectively. We denote $n$-dimensional vector spaces over $\R$ and $\C$ by $\R^{n}$ and $\C^{n}$ respectively. We use $\R^{\ncn}$ and $\C^{\ncn}$ to denote $\ncn$ matrices with real and complex entries respectively. For any $x\in \C$ we denote the modulus of $x$ by $\md{x}=\sqrt{\re{x}^2+\im{x}^2}$. For any $x\in \C^n$We denote complex conjugate of $x$ by $\bar{x}$. We denote the transpose of $x\in \C^{n}, A\in \C^{\ncn}$ by $x^\top$ and $A^\top$ respectively. Further, by $x^\dag$ denotes the conjugate transpose, i.e., $x^\dag=\bar{x}^\top$ (the same convention carries over to matrices as well). We will use $\ip{\cdot,\cdot}$ to denote the inner products: $\ip{x,y}=x^\dag y$. Also, by $A_{ij}$ we denote the entry of matrix $A$ in the $i^{th}$ row and $j^{th}$ column.
The norm of the matrix $A$ in given by $\norm{A}\eqdef \sup_{x\in \C^n:\norm{x}=1} \norm{Ax}$. For a positive real number $B>0$, we denote by $\C^{\ncn}_B=\{A\in \C^{\ncn}\mid \norm{A}\leq B\}$ and  $\R^{\ncn}_B=\{A\in \R^{\ncn}\mid \norm{A}\leq B,\,\forall i,j=1,\ldots,n\}$ the set of complex and real matrices whose norms are bounded $B$.
%We denote by $\C^{\ncn}_B=\{A\in \C^{\ncn}\mid \md{A_{ij}}\leq B,\,\forall i,j=1,\ldots,n\}$ and  $\R^{\ncn}_B=\{A\in \R^{\ncn}\mid \md{A_{ij}}\leq B,\,\forall i,j=1,\ldots,n\}$.
%We use $\P$ to denote the set of probability distribution over $\C^{\ncn}$.
We also denote the set of invertible $\ncn$ complex matrices by $\gln$. We use $A\sim P$ to denote the fact that the random variable $A$ is distributed according to distribution $P$. Let the random matrix $A\sim P$, and $U\in \gln$, we use $P_U$ to denote the distribution of the random matrix $U^{-1}A U$.  We denote the identity matrix in $\C^{\ncn}$ by $\I$.
%We denote the set of distributions over $\C_B^{\ncn}$ by $\P$.

We use $A\succeq 0$ to denote that the
square matrix $A$ is hermitian and positive semidefinite (HPD):
$A = A^\dag$, $\inf_x x^\dag A x\ge 0$.
For $A,B$ HPD matrices, $A\succeq B$ holds if $A-B\succeq 0$.
We also use $A\succ B$ similarly to denote that $A-B \succ 0$.
We also use $\preceq$ and $\prec$ analogously. For any $x\in\C^n$x, we denote the general quadratic norm with respect to a $C\succ 0$ by $\norm{x}^2_C\eqdef x^\dag\, C \,x$.
$\E$ denotes mathematical expectation.

\begin{definition}
Let $P$ be a distribution over $\C^{\ncn}$, then define
\begin{align*}
A_P=\int M dP(M),\quad C_P=\int M^\dag M dP(M),
\end{align*}
\end{definition}
\begin{definition}\label{contract}[Contraction Factors]
Define
\begin{align*}
\rhod{P}&\eqdef {\inf}_{x\in\C^n\colon\norm{x}=1}\ip{x,\left((A_P+A_P^\dag)-\alpha A_P^\dag A_P\right)x},\\ \rhos{P}&\eqdef{\inf}_{x\in \C^n\colon\norm{x}=1}\ip{x,\left((A_P+A_P^\dag)-\alpha C_P\right)x}
\end{align*}
\end{definition}
Note that $\rhod{P}>\rhos{P}$.

\begin{definition}
We call a matrix $A\in \C^{\ncn}$ to be \emph{positive definite} (PD) if $\ip{x,Ax} >0,\,\forall x\neq 0 \in \C^{n}$. We call a matix $A\in \C^{\ncn}$ to be \emph{asymptotically stable} (AS) if all eigenvalues of $A$ have positive real parts. We call a matrix $A\in \C^{\ncn}$ to be \emph{symmetric positive definite} (SPD) is it is symmetric i.e., $A^\top=A$ and PD.
\end{definition}
\begin{definition}\label{distpd}
We call a distribution $P$ to be PD/AS/SPD if $A_P$ is PD/AS/SPD.
\end{definition}


\section{Problem Setup}
We consider the linear stochastic approximation (LSA) algorithm with constant step size and PR-averaging of the iterates.
\begin{subequations}\label{eq:lsa}
\begin{align}
\label{conststep}&\text{LSA:} &\theta_t&=\theta_{t-1}+\alpha(b_t-A_t\theta_{t-1}),\\
\label{iteravg}&\text{PR-Average:} &\thh_t&=\frac{1}{t+1}{\sum}_{i=0}^{t}\theta_i,
\end{align}
\end{subequations}
where $\thh_t, \theta_t \in \C^{n}$. The algorithm updates a pair of parameters $\theta_t,\tb_t\in \C^{\ncn}$ incrementally, in discrete time steps $t=1,2,\dots$
based on data $b_t\in \C^n$, $A_t\in \C^{n\times n}$\footnote{In almost all practical instances, the iterates $\theta_t,\thh_t\in \R^n$ and $A_t\in \R^{\ncn}, b_t\in \R^n$. However, it serves well for our analysis to make the general assumption that these qunatites are complex valued.}. Here $\alpha>0$ is a positive step-size parameter; the only tuning parameter of the algorithm besides the
initial value $\theta_0$. The iterate $\theta_t$ is treated as an internal state of the algorithm, while $\th_t$ is the output at time step $t$. The update of $\theta_t$ alone is considered a form of constant stepsize LSA. Note that the computation of $\theta_t$ can also be performed in an incremental fashion using $O(n)$ storage.

Owning to the special form of $A_t$, sometimes the matrix-vector product $A_t \theta_{t-1}$
can be efficiently computed in $O(n)$ time.
This happens for example when $A_t$ is rank one; examples of this kind arising in reinforcemnet learning \cite{} and linear prediction \cite{} will be presented in a later sections (\Cref{sec:rl,sec:lp}). The significance of efficient computation of the matrix-vector product is that then an update of the algorithm
can be implemented in $O(n)$ time and $O(n)$ storage, which makes the algorithm particularly attractive in large-scale computations when $n$ is in the range of thousands or more.
We are interested in the behaviour of \eqref{eq:lsa} under the assumptions presented in \Cref{assmp:lsa} In what follows, we make use of the $\sigma$-field given by $\F_{t-1}=\{\theta_0,A_1,\ldots, A_{t-1}, b_1,\ldots, b_{t-1}\}$.
\begin{assumption}\label{assmp:lsa}
\begin{enumerate}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item \label{dist} $\{A_t\in\C^{\ncn},\, t\geq 0\}$ is an $i.i.d$ sequence distributed according to $P$ which is either AS or PD or SPD.
\item $\{b_t\}$ is an $i.i.d$ sequence of vectors in $\C^n$ with mean $\E[b_t]=b$.
\item \label{matvar} The martingale difference sequences\footnote{$\EE{M_t|\F_{t-1}}=0$ and $\EE{N_t|\F_{t-1}}=0$} $M_t\eqdef A_t-A_P$ and $N_t\eqdef b_t-b$ associated with $A_t$ and $b_t$ satisfy the following
%\begin{align*}\E\left[ M_t^\dag M_t\mid\F_{t-1}\right]\leq \Sigma^2_P, \, \E[N_t^\dag N_t]=\sigma_b^2.\end{align*}
\begin{align*}\E\left[ \norm{M_t}^2\mid\F_{t-1}\right]\leq \sigma^2_P, \, \E[N_t^\dag N_t]=\sigma_b^2.\end{align*}
\item $A_P$ is invertible and there exists a $\ts=A_P^{-1}b$.
\end{enumerate}
\end{assumption}
\textbf{Performance Metric:}
The ``aim'' of the updates in \eqref{eq:lsa} is to estimate $\ts$ based on the noisy versions $(A_t,b_t)$ of $(A_P,b)$.
There are various ways of quantifying the error $\eh_t \doteq \thh_t- \ts$ of estimating $\ts$ using $\thh_t$, but the magnitude
of the errors is usually measured in some weighted quadratic norm. The expected loss for time step $t$ then becomes
\begin{align*}
L_t \doteq \EE{ \norm{\eh_t}_{C}^2 }\,,
\end{align*}
where recall that for a SPD matrix $C$, $\norm{x}_C^2 = x^\dag C x$. Here, the choice of the matrix $C$ depends on the application.\\
\textbf{Challenges:}
The idea of constant step size and iterate averaging is not new and goes back to \cite{}, where it was first shown that $L_t\ra 0$ as $t\ra\infty$. Recently \cite{bach} revisited the idea in connection to the stochastic gradient descent (SGD) algorithm (which is also an LSA of the form in \eqref{eq:lsa}) to solve linear regression. Much similar to the spirit of \cite{bach}, our aim here is to understand the aspects that effect the behaviour of $L_t$, such as:
\begin{itemize}
\item The variances $\sigma_P^2$ and $\sigma_b^2$ of the noise terms $M_t$ and $b_t$ in the update equation.
\item The baias $\norm{\theta_0-\ts}$ due to the initial condition $\theta_0$.
\item The effect of the structural properties of the $A_P$, i.e., whether it is AS or PD or SPD.
\end{itemize}
Most resutls in the past including those that of \cite{bach} consider an SGD algorithm for quadratic loss minimization, where the matrix $A_P$ is SPD and the noise has additional strucutral properties (we discuss this briefly in \Cref{sec:lp}). As it turns out these nice properties are lost when $A_P$ is AS or PD (a property that is holds in RL settings). While from the standpoint of the tools and techniques used, we use techniques much similar to \cite{bach}, however, as mentioned, the fact that we no longer are dealing with gradient systems (that involve to SPD matrices) but general vector fields (that involve AS or PD matrices), poses important challenges.
Many RL algorithms \cite{} are LSA algorithms that need only $O(n)$ updates. However, there is a lot of variability in their analyses. The analysis as well as the results in this paper are useful from the perspective that it presents a single unified way of understand the various LSA-RL algorithms. We now present and discuss the main results of this paper in the next section. The comparison with existing literature is done in two separate sections, one to compare our results with \cite{bach} in the linear predictions setting (\Cref{sec:lp}) and the other to compare our results with prior results in RL literature \cite{}.
\todoch{Mention impotance of forgetting the bias etc. Mention that we really don't have control over $\norm{U}$}
\section{Main Results}
\begin{lemma}
Let $P$ be a positive definite distribution over $\C^{\ncn}$, then there exists an $\alpha_P$ such that $\rhod{P}>0$ and $\rhos{P}>0,~\forall \alpha \in (0,\alpha_P)$.
\end{lemma}
\begin{theorem}
Let $P$ in \Cref{lsadist} be a positive definite distribution over $\C^{\ncn}$. Then
\begin{align}
\EE{\norm{\thh_t-\theta^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P}}\right)\frac{1}{\alpha \rhos{P}}
\left(\frac{\norm{\theta_0-\theta^*}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\theta^*}}{t+1} \right)\,.
%\left(\frac{\norm{\theta_0-\theta^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\theta_0-\theta^*})}{t+1} \right)\,.
\end{align}

\end{theorem}

\begin{lemma}[Similarity Transformation]
Let $P^{AS}\in\P$ be asymptotically stable, then there exists a $U\in \gln$ such that $P^{AS}_U$ is positive definite.
\end{lemma}

\begin{theorem}[Change of Basis]
Let $P$ be as in \Cref{lsadist}. Define $\gamma_t\eqdef U^{-1}\theta_t,\,,\gamma^*\eqdef U^{-1}\theta^*$, and suppose if $P_U$ is positive definite, then
\begin{align}
\EE{\norm{\gh_t-\gamma^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U^{-1}}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\gamma_0-\gamma^*}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\gamma_0-\gamma^*})}{t+1} \right)\,,
\end{align}
where $\gh_t=\frac{1}{t+1}\sum_{s=0}^t \gamma_s$.
\end{theorem}
\begin{corollary}
Let $P$ in \Cref{lsadist} be asymptotically stable. Then, there exists
\begin{align}
\EE{\norm{\thh_t-\theta^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U}^2\norm{U^{-1}}^2}{\alpha \rhos{P_U}} \left(\frac{\norm{\theta_0-\theta^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\theta_0-\theta^*})}{t+1} \right)\,.
\end{align}
\end{corollary}

\section{Uniform stepsizes}
\begin{definition}
Call a set of distributions $\P$ over $\C^{\ncn}$
\emph{weakly admissible} if there exists $\alpha>0$ such that
$\rhos{P}>0$ holds for all $P\in \P$.
\end{definition}
\begin{definition}
Call a set of distributions $\P$ over $\C^{\ncn}$ \emph{admissible}
if there exists some $\alpha>0$ such that $\inf_{P\in \P} \rhos{P}>0$.
The value of $\alpha$ is called a witness.
\end{definition}
It is easy to see that $\alpha \mapsto \rhos{P}$ is decreasing,
hence if $\alpha>0$ witnesses that $\P$ is (weakly) admissible
then any $0<\alpha'\le \alpha$ is also witnessing this.

If $\P$ is weakly admissible, then one can choose a stepsize $\alpha>0$ solely based on the knowledge of $\P$ and
conclude that no matter the joint distribution of $(A_t,b_t)$, as long as the distribution of $A_t$ is an element of $\P$, the conclusions of \cref{maintheorem} hold. Further, if $\P$ is admissible then the error bound stated in  \cref{maintheorem} becomes independent of the instance.
Hence, an interesting question to investigate is whether a given set $\P$ is (weakly) admissible.
A hopeful set is the following:
For $B>0$, let $\P_B$ be the set of distributions over $\C_B^{\ncn}$ where for every $P\in \P_B$ is positive definite.
In what follows all distributions will be a subset of $\P_B$ with some $B>0$,
Note that in most applications the knowledge of a suitable $B>0$ such that $\P\subset \P_B$ is available.
By appropriately normalizing the matrices involved, we may as well assume that $B=1$.

Our first result shows that we cannot be too ambitious:
\begin{proposition}\label{notwad}
The set $\P_1$ is not weakly admissible.
\end{proposition}
\begin{proof}
Fix an arbitrary $\alpha>0$. We show that there exists $P\in \P$ such that $\rho_\alpha(P)<0$.
For $\epsilon \in (0,1/2)$ let $P$ be the distribution that is supported on $\{-I,I\}$ and takes on the value of $I$ with probability $1/2+\epsilon$. Then $A_P = 2\epsilon I \succ 0$, hence $P\in \P_1$. Further, $Q_P = I$.
Hence, $\rhos{P} = 4\epsilon-\alpha$. Hence, if $\epsilon<\alpha/4$, $\rho_\alpha(P)<0$.
\end{proof}
Thus \Cref{notwad} positive definiteness of $P\in \P_B$ alone is not sufficient for weak admissibility.
In linear least-squares problems (cf. \cref{ex:leastsquares}) the set $\P$ has more structure.
In particular, the random matrices $\{A_t\}$ are PSD. \todoc{The standard notation is $\mathrm{S}^{++}$ for the set of positive definite matrices. Introduce these and use them below..}
Define
\begin{align*}
\P_{\text{PSD},B} = \{ P \in \P_B \,:\,  \supp(P)\subset \text{PSD} \}\,.
\end{align*}
We have the following proposition:
\begin{proposition}
For any $B>0$, the set $\P_{\text{PSD},B}$ is weakly admissible
and in particular any $0<\alpha < 2/B$ witnesses this.
\end{proposition}
\begin{proof}
Take any $P\in \P_{\text{PSD}}$ and let $H\sim P$.
Consider the SVD of $H$: $H = U \Lambda U^\top$ where $U$ is orthonormal and $\Lambda$ is diagonal with
nonnegative elements. Note that $\Lambda \preceq B\, \I$ and thus $\Lambda^2 \preceq B \Lambda$.
Then for any $x\in \R^n$, $x^\top H^\top H x = x^\top U \Lambda^2 U^\top x \le B x^\top U \Lambda U^\top x = B x^\top H x$.
Taking expectations we find that $x^\top Q_P x \le B x^\top H_P x$.
Hence, $\rho_\alpha(P) = 2 x^\top H_P x - \alpha x^\top Q_P x \ge (2- \alpha B ) \,x^\top H_P x $.
Thus, for any $\alpha<2/B$, $\rho_\alpha(P)>0$.
\end{proof}
\todoc[inline]{So this altogether is weaker than what Bach founds. They prove strong admissibility.
I am guessing that for this they use that the loss is special and also that the noise is special.
This would be a good place to somehow incorporate this with a (short?) proof.
}
\subsection{Result Discussion}

For more details see the discussion on related work section


\section{Special Cases}
\subsection{Additive Noise Case}

\subsection{Symmetric Matrix}

\subsection{Linear Prediction With Structured Noise}


\section{Reinforcement Learning}

\section{Stochastic Approximation and Mirror Prox}



\begin{comment}
\paragraph{Contraction over time $t$}
The condition of contraction per time step is overly restrictive in the general case (other than the one presented in \Cref{rankonestep}). We can relax this condition and ask whether a weaker result in \Cref{multheorem} holds.
\begin{lemma}\label{multheorem}
Let $\P$ be the space of positive definite distributions supported on $\C^{\ncn}$. Then there exists an $\alpha_{\max}>0$ such that $\alpha\in(0,\alpha_{\max})$ implies $\us{\sup}{\norm{x}\leq 1,P\in\P,t\geq 0}\E_{P}[\norm{(I-\alpha H)^tx}]< \infty$.
\end{lemma}
\textbf{Possible Counter example for \Cref{multheorem} in $\R$}
Consider $\H=[-1,1]$ and $\P$ supported on $\{-1,1\}$ with $P(1)=\frac{1}{2}+\epsilon$ and $P(-1)=\frac{1}{2}-\epsilon$. Note that
\begin{align*}
\E[\Pi_{i=1}^t (1-\alpha H_i)]&=\exp{\ous{\sum}{i=1}{t}\log(1-\alpha H_i)}\\
&\approx \exp{t\E[\log(1-\alpha H_i)]}
\end{align*}
Now the condition $\E[\Pi_{i=1}^t (1-\alpha H_i)]<\infty,$ where $H_i\sim P$ loosely translates to the following
\begin{align*}
\E[\log(1-\alpha H_i)]<0,
\end{align*}
which can be verified by noting that
\begin{align*}
&\E[\log(1-\alpha H_i)]\\&=\E[(\frac{1}{2}+\epsilon)\log (1-\alpha)+(\frac{1}{2}-\epsilon)\log (1+\alpha)]\\
&=\frac{1}{2}\log(1-\alpha^2)+\epsilon(\log(1-\alpha)-\log(1+\alpha))\\
&<0
\end{align*}
\textbf{Possible Counter example for \Cref{multheorem} in $\R^2$}
Given any $\alpha$ the aim is to choose a distribution $P$ over $\H$ such that $\E_{P}[\norm{(I-\alpha H)^tx}]$ grows with $t$. In other words, given an $\alpha>0$, we are looking for a distribution $P'$ supported in the $\alpha$ neighbourhood of $I$ denoted by $\F$ such that $\E_{P'}[F^t]$ grows with $t$ (where $F\sim P'$). We now look at a possible suggestion for such a $P'$.\par
\begin{example}
Pick $F_1=\begin{bmatrix} 0 & 1-\beta_2 \\ 1+\beta_1& 0\\\end{bmatrix}^{\frac{1}{p}}$, $F_2=\begin{bmatrix} 0 & 1+\beta_1 \\ 1-\beta_2& 0\\\end{bmatrix}^{\frac{1}{p}}$, where $\beta_2>\beta_1$ such that $1+\frac{1}{2}(\beta_1-\beta_2)$. Let $P'$ be supported on $\{F_1,F_2\}$ with $P'(F_1)=\frac{1}{2}$ and $P'(F_2)=\frac{1}{2}$, and  $p$ is sufficiently large such that $F_1,F_2$ belong to the $\alpha$-neighbourhood of $I$. A possible way the random product can blow up is when products $F_2^pF_1^p$ occur in a contiguous stream. To see this,
\begin{align}
F_2^pF_1^p=\begin{bmatrix} (1+\beta_1) &0  \\ 0& (1-\beta_2)^2\\\end{bmatrix}
\end{align}
Thus $\norm{F_2^p F_1^p}>1$. However, such (i.e., $F_2^p F_1^p$) sequences have probability of occurance $\frac{1}{2^{2p}}$ is a sequence of length $2p$. Thus probability of them occuring quite often in a given random product is small.
\end{example}
\todoc[inline]{Did not we also want to discuss the multiplicative ergodic theorem? I have not cleaned this part yet..}
\end{comment}

\appendix
\input{linal}

\begin{comment}
\input{linal}
\section{Problem Setup}
We consider linear stochastic approximation schemes with constant step size and PR-averaging given as below:
\begin{align}
\theta_t&=\theta_{t-1}+\alpha(b_t-A_t\theta_{t-1}),\\
\thh_t&=\frac{1}{t+1}{\sum}_{i=0}^{t}\theta_s
\end{align}
where we assume
\begin{assumption}\label{lsa}
\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]
\item $A_t=A+M_t$ and $b_t=b+N_t$, where $\{M_t \in \R^{\ncn},\,t\geq 0\}$ and $\{N_t\in\R^n,\,t\geq 0\}$ are $i.i.d$ sequences such that $\E[M_t]=0$ and $\E[N_t]=0$.
\item There exists a $\ts\in \R^n$ such that $\ts=A^{-1}b$.
\item All the eigenvalues of $A$ have strictly positive real parts.
\item $\E[M_t^\top M_t\ts]=\sigma_1^2$ and $\E[\norm{N_t}^2]=\sigma_2^2$.
\end{itemize}
\end{assumption}
\Cref{lsa}
\begin{example}[Linear Least Squares]
\end{example}

\begin{example}[TD(0)]
\end{example}


\section{Notation}
Let $\R^{\ncn}$ and $\C^{\ncn}$ denote the set of $n\times n$ real and complex matrices respectively. Let $\gln\subset\C^{\ncn}$ denote the set of all invertible $\ncn$ complex matrices.



\section{Definitions}
\begin{definition}[Asymptotically Stable]
An $\ncn$ matrix said to be \emph{asymptotically stable} iff all its eigenvalues have positive real parts.
\end{definition}

\begin{definition}\label{def:dist}[Distributions]
Given a distribution $P$ over $\C^{\ncn}$ and let
\begin{align*}
A_P=\int M dP(M),\quad C_P=\int M^\dag M dP(M),
\end{align*}
and let $\P$ denote the set of probability distributions on $\C^{\ncn}$.
\end{definition}

\begin{definition}[Similarity Transformation]
Given any $U\in \gln$, define $\T_U \colon \C^{\ncn}\rightarrow \C^{\ncn}$ be the linear map that takes $A\in \C^{\ncn}$ to $\T_U(A)=U^{-1}\,A\,U$.
\end{definition}
\begin{definition}[Transformed Measure]
Given a distribution $P$ over $\C^{\ncn}$ define the distribution $P_U$ induced by the linear transformation $\T_U$ as follows:
\begin{align*}
\int_{S} dP_U(M) \eqdef \int_{\T_U^{-1}(S)} dP(M)
\end{align*}
\end{definition}


\begin{definition}[Induced Distribution]
Let $P\in \P$ be any distribution over $\R^{\ncn}$ and let $p_c^U$ denote the probability distribution over $\C^{\ncn}$ induced by the linear map $\T_U$ such that the pullback of $p_c^U$ using $\T_U$ is $p$.
\end{definition}



\begin{definition}[Weakly Admissible]
A set of distributions $\P$ over $\C^{\ncn}$ is said to be \emph{weakly admissible} if for any given $P\in \P,\,\exists$ a matrix $U_P \in \gln$, real-valued constants $\alphapd>0$ and $\alphaps>0$ such that $\rhod{P_{U_P}}>0,\,\forall \alpha\in(0,\alphapd)$ and $\rhos{P_U}>0,\,\forall \alpha\in(0,\alphaps)$.
\end{definition}
\begin{definition}
Given a weakly admissible class $\P$ and any $P\in\P$, $U\in \gln$ and $\alpha>0$ such that $\rhos{P_{U}}>0$ define
\begin{align*}
\bu&\eqdef \norm{U}^2\norm{U^{-1}}^2 \left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{1}{\alpha \rhos{P_U}}\\
\beta(P)&\eqdef \inf_{\alpha,U\colon \bu>0} \bu
\end{align*}
\end{definition}



\begin{definition}[Admissible]
A set of distributions $\P$ over $\C^{\ncn}$ is \emph{admissible} if  for any given $P\in \P,\,\exists$ a matrix $U_P\in \gln,\,$ and a real-valued constant $\alpha_0>0$ such that $\rhod{P_{U_P}}>0,\,\forall \alpha\in(0,\alpha_0)$ and $\rhos{P_{U_P}},\,\forall \alpha\in(0,\alpha_0)$.
\end{definition}
\begin{example}
\end{example}
\begin{definition}[Strongly Admissible]
A set of distributions $\P$ over $\C^{\ncn}$ is \emph{strongly admissible} if  $\forall\,P\in \P,\,\exists U_P\in \gln,\, \alpha_0>0$ such that $\rhod{P_{U_P}}>0,\,\forall \alpha\in(0,\alpha_0]$ and $\rhos{P_{U_P}},\,\forall \alpha\in(0,\alpha_0]$, $\inf_{P\in \P} \rho_d(\alpha_0,P_{U_P})>0$ and $\inf_{P\in \P} \rho_s(\alpha_0,P_{U_P})>0$.
\end{definition}
\begin{example}
\end{example}

\begin{lemma}\label{pdt}
Given a AS matrix $A$, there exists a non-singular matrix $U\in \C^{\ncn}$ such that $A=U\Lambda U^{-1}$ where $\Lambda^\dag+\Lambda$ is a real symmetric positive definite matrix.
\end{lemma}
Note that in \cref{pdt} the transformation $U\Lambda U^{-1}$ is not unique.
\begin{example}
\end{example}


\begin{lemma}
Given any AS matrix $A$, there exists $\alpha_D>0,\,\alpha_R>0$ and a non-singluar matrix $U$ such that
\begin{enumerate}
\item $\rhod{\Lambda_1}>0,\,\forall \alpha \in (0,\alpha_D)$.
\item $\rhos{\Lambda_1}>0,\,\forall \alpha \in (0,\alpha_R)$.
\end{enumerate}
\end{lemma}
\begin{corollary}
There exists $\beta\eqdef\inf_{U : det(U)\neq 0,\alpha\in (0,\alpha_R)} \bu$.
\end{corollary}
\section{Main Results}
The following theorem states that weak admissibility implies stabilizability.
\begin{theorem}
Let $\P_{as}$ be a set of distributions supported on $\R^{\ncn}$ such that for any given $P\in \P$, all the eigenvalues of $A_{P}$ have positive real parts. $\P_{as}$ is \emph{weakly admissible}.
\end{theorem}
In short, so long as the matrices $A_t$ are distributed such that all the eigenvalues of $\E[A_t]$ have with positive real parts, then there always exists a positive step size $\alpha$ (albeit problem dependent) that results in \eqref{lsa} to converge in the mean-squared sense.

In \Cref{thm:rate} that we state we below, we let $P^A$ denote the underlying distribution that generates $A_t, t\geq 0$, and in order to be consistent with \Cref{lsa} and \Cref{def:dist} it is understood that $A_{P^A}=A$.
\begin{theorem}\label{thm:rate}
\begin{align}
\E[\norm{\zb_t}^2]
\leq \beta(P^A)
\left(\frac{\norm{e_0}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{e_0})}{t+1} \right)\,.
\end{align}
\end{theorem}

\section{Comparison With Prior Works}

\subsection{Stochastic Approximation}

\section{Interesting Special Cases}

\section{Application in Reinforcement Learning}
\subsection{RL Setting}

\subsection{TD(0) Algorithm}


\subsection{GTD}
\textbf{Flaws with GTD-MP:}

\textbf{Why is GTD slow:}


\subsection{iLSTD}

\subsection{Comparison of TD, GTD and iLSTD}

\section{Conclusion}


\appendix

\input{linal}

\end{comment}

\end{document}
